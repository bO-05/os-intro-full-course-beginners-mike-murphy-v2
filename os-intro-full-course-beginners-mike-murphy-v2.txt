# TIMESTAMPS:

Table of contents:
0:00:00 Introduction to Operating System
0:12:30 Hardware Resources (CPU, Memory)
0:24:54 Disk Input & Output
0:37:08 Disk Scheduling 
0:49:38 Development Cycles
0:58:21 Filesystems 
1:08:14 Requirements Analysis
1:17:40 CPU Features
1:24:55 Kernel Architectures
1:35:15 Introduction to UML (Unified Modeling Language)
1:42:20 UML Activity Diagrams
1:50:22 Interrupts and I/O
1:57:40 Interrupt Controllers
2:05:47 Use Cases
2:15:51 Interrupt Handling
2:24:53 UML State Diagrams
2:35:42 Dynamic Memory Allocation
2:42:10 Kernel Memory Allocation
2:49:50 Memory Resources
2:56:30 Paging
3:03:26 Memory Protection
3:11:24 Test Driven Design
3:22:22 Page Tables
3:31:29 UML Class Diagrams
3:40:11 Virtual Memory
3:49:22 Object-Oriented Design
3:57:47 Object-Oriented Implementations
4:13:07 Page Replacement
4:20:20 Processes


----------------------------------------------------
----------------------------------------------------


# TRANSCRIPTS - generated by Deepgram: 

Speaker 0:

Sleep and study. Don't forget to subscribe. In this lecture, I'm going to introduce the fundamentals of operating systems. What they are, what they do, and why they're important. Now what's an operating system?

Well, it's a layer of software that provides 2 important services to a computer system. It provides abstraction and arbitration. Abstraction means hiding the details of different hardware configurations so that each application doesn't have to be tailored for each possible device that might be present on the system. Arbitration means that the operating system manages access to shared hardware resources so that multiple applications can run on the same hardware at the same time without interfering with one another. These hardware resources that need to be managed include the CPU, the hierarchy of memory, all the input output devices on the system, and to some degree, the power and system management features in the system.

Many of these features are handled directly by the hardware, but the operating system is involved particularly in energy conservation. Now the abstraction features of the operating system allow hardware devices manufactured by different manufacturers to have the same interface within software for applications to use. These hardware devices all have different low level instruction sets. They all have particularly particular capabilities, features, and details that are unique to each hardware device. If we didn't have a common interface into these hardware devices, first of all, our variety of hardware might be limited.

But worse, every application on the system would have to be programmed to use every single device on the system. An example, back in the 19 nineties, computer games often required internal programming for specific video cards and specific sound cards. It was often necessary to go into the settings for each game and tell the game what type of video card you had, what type of sound card you had, for the purpose of configuring the game to use that particular hardware. Imagine if that had to be done for every single application on the system, including, say, a calculator or a web browser. That would be a very untenable situation in terms of being able to make use of computers the way we use them today.

Also, what if we could only run 1 program at a time on a computer system? Years years ago, that was the case. However, a modern system is running multiple applications simultaneously, and it's up to the operating system to ensure that all these applications can access resources. So each CPU is divided among the different programs. Each program gets access to memory, input output, as well as disk.

And in an ideal world, the operating system also enforces policies that isolate applications from each other so that a crash in one application doesn't take down the entire system or other applications. Now, of these examples, do we have a situation where we have abstraction or arbitration? Well, first example supporting both Intel and AMD Processors. This is an example of abstraction. We don't have to write separate software for an Intel Processor relative to an AMD processor, at least for 99.99 percent of applications.

Simply write the application once, it will run on either process. Switching between applications is an example of arbitrating hardware resources among the different applications on the system. Separating memory allocated to different applications is also an arbitration activity. It keeps one application from overriding the contents of memory that's being used by another application. Enabling video conferencing software to use different camera devices.

This would be an example of abstraction. The video conferencing program just has to know how to use a camera interface that the operating system provides, and then different cameras from different manufacturers can be used without having to write the application, the video conferencing program, to be able to talk to each individual camera. Similarly, accessing 2 different hard disks from 2 different manufacturers. Any underlying detail differences between those drives can be handled by using the operating system to abstract away the details. Sending and receiving messages over a network is both abstraction and arbitration.

On the one hand, we're abstracting away the details of each particular network card to be able to send and receive the message. On the other hand, we're sharing that network card among all the different applications on the system. Now we can think of the system in terms of layers. At the bottom of the layer cake, we have the hardware. This is what executes the software, the top three layers.

The operating system is the middleman, so to speak, between the applications and the libraries and utilities upon which the applications depend and the underlying hardware. Specifically, the core of the operating system or the kernel, so named after, say, a kernel of corn, is the minimum piece of software that's needed to share the hardware between the different applications. Whatever lives outside the kernel that is software is said to be in user space. That means it's application code or sometimes even parts of the operating system that are not strictly necessary to share the hardware or abstract away its details. Now operating systems implement a common mechanism for allowing applications to access the hardware, which is abstraction, And the applications make requests from the operating system to go and access the hardware or other features by making system calls down into the operating system.

This is called entering the operating system or entering entering the kernel from the top half. Operating systems are also able to alert applications that hardware has changed. Perhaps a network packet has come in. Perhaps the user has pressed a key on the keyboard. These alerts are delivered via a system called interrupts and refer to entering the kernel from the bottom half, from the hardware side of the operating system.

We should also note that operating systems can manage and terminate applications by sending signals to those applications. Now there are a wide variety of operating systems out there. They basically fall into 2 categories, Microsoft Windows, which is not UNIX, and everything else, which is UNIX. Microsoft Windows systems, non UNIX systems, are the most popular desktop operating systems at the moment. However, they're rapidly being eclipsed by tablet devices, most of which are running UNIX style systems.

Windows is typically pre installed by PC manufacturers, which accounts for a good portion of its present popularity. UNIX systems, on the other hand, are installed by the end user, except for commercial UNIX systems and Mac OS 10. Mac OS 10 is probably the best selling UNIX system of date, and, uh, Linux, which includes the Android platform, uh, is probably second. Now there are some mainframe systems out there also, some of which are Unix like. Some use custom operating systems.

There are a variety of players in the embedded systems market. However, that's presently dominated by Android with Apple's iOS, which is based on Mac OS 10, in close second. Others, Symbian, Blackberry OS, TinyOS are confined to smaller markets. Now the idea behind the UNIX systems began with an a time sharing system started in 1964 called Multics, and the idea behind Multics was to make computing into a remote service that could be accessed by terminals using telephone lines. It wasn't terribly successful because dial up connections at the time at about 96 100 BOG were rather slow.

However, Canada actually did have a Multics system deployed and used it as part of their national defense network until the year 2000. Multics is remembered, however, primarily for its influence as a multi user shared system. And today, with cloud computing, there's some significant parallels between some of the cloud computing models that we use and some of the ideas that were pioneered back in the Multics system. Now UNIX was actually a play on the term Multics. It was developed by Bell Labs in 1969.

It's actually a trademark term, so some authors use starnex to refer to the family of systems and use the word UNIX only for the commercial distributions. The original UNIX systems were commercial distributions with free variance BSD and Linux coming along late eighties early nineties. The UNIX systems are time sharing systems in that they support multiple users at the same time, but they're designed to be run on local computer resources instead of remote resources. The Berkeley system distribution, which is based on AT and T's commercial unit systems to emerge. It was based on the commercial distribution, so AT and T sued UC Berkeley, but an eventual settlement allowed Berkeley to distribute BSD freely in both source and binary form.

Today's descendants of BSD include FreeBSD, OpenBSD, and Mac OS 10, which is based on the Unix BSD system. Linux, however, was an alternate approach to getting a Unix like kernel running on a computer system. This was started by Linus Torvalds as an undergrad at the University of Helsinki, released in source code form in 1991, and often combined with a set of user space, utilities, and libraries created by the GNU project. Thus, the resulting combination is sometimes called GNU slash Linux. It has been commercially successful for many device classes, including web servers, network and embedded devices, and mobile phones.

Some people, including myself, use Linux as a desktop platform, and it does have benefit of having the one kernel that's scalable from embedded devices all the way up to supercomputers. Now it's convenient to think of a system, computer system, as a set of layers, where at the top layer we have a group of applications. These are the programs that we run and with which we interact on a day to day basis to accomplish various tasks. These applications are built on top of libraries and utilities, some of which are included with the operating system and some of which are installed separately. The operating system acts as an intermediary layer between the application layer and utilities and the underlying hardware.

The underlying hardware consists of several important pieces. I'll talk more in detail about these in a moment. Two primary pieces that come to mind are the central processing unit, which actually executes the program instructions, and the memory hierarchy, which provides a place to store instructions and data for use in computation. Now the memory hierarchy is critical to understand because the speeds of each level of memory within the hierarchy vary. The fastest memory consists of CPU registers, and then the cache attached to the CPU itself, whereas the slowest memory consists of persistent memory, typically disk.

The hardware also includes a bunch of input output devices. You have the keyboard, mouse, network interface card, a screen, printer, plenty of others, webcam, different types of devices. One of the other things that the hardware does is supplies power to the system via power supply and removes heat generated by the system through cooling, which is typically done through fans built into the computer's case. Now when we look at a computer system from the outside, we see something called a form factor. A form factor is simply a set of specifications for standard sizes of equipment to allow different computer components developed by different people to be interchanged with one another.

And standard form factors include ATX, which is a typical workstation size form factor, microATX, and mini ITX, which are used in small form factor systems, such as bookshelf systems and home theater PCs, and rackmount systems, which are typical servers in the data center. Now laptop systems are a special case. These types of systems typically are proprietary in nature to the extent that it's not easy to take components out of 1 laptop and put it in another laptop. Workstation systems, however, do allow easy component interchange, and these are the simplest of systems. They typically support multiple users, but normally only 1 user at a time.

When a workstation is shared between different users, it often involves a user getting up from the keyboard and another user sitting down. Of course, it's not always the case, but generally that's the case. Workstation systems are also frequently either ATX or microATX form factors, and each workstation typically has its own power supply, which can be an issue of efficiency considering the power supplies are only about 65 to 85 percent efficient. Workstations can be used to host server applications. This is particularly true in smaller settings such as small businesses, nonprofit organizations.

Server applications in larger scale environments, such as enterprise environments, are typically hosted in rack mount systems. And rack mount systems are simply space efficient way to put a whole lot of servers into 1 data center. Rack mount systems are measured or rack sizes are measured in terms of units or u. In a typical rack's 42 u tall, so the most space efficient of servers, a 1 u system with 12 cores in each machine, could enable up to 480 cores 480 CPU cores to be supported by a single rack. However, there won't be much room to for expansion.

There won't be much room for extra devices or extra hard drives. For that setup, we use 2 u and 4 u systems, which are less space efficient, but do allow for more expansion, more disks to be included, and more add in guards. Now if we take a computer system and we open up the case, inside we're going to find a power supply, we're gonna find a motherboard or what Apple calls a logic board, and attached to that motherboard we're gonna find the CPU, we're gonna find random access memory, we're gonna find expansion slots for add in devices, some of which may be populated, and we're gonna find that the motherboard contains its own onboard device, such as network controllers or video controllers. Inside the case, we're also gonna find persistent storage devices, such as hard disks, solid state drives, and optical drives. And here's a conceptual view of what the inside of the computer system looks like.

Here we have the motherboard, which has some expansion slots. These are normally used for add in cards to be plugged in, add functionality to the system, onboard devices, which are soldered onto the motherboard and are not removed, 1 or more CPUs, which are typically removable. These are typically separate chips. And each of these chips inside you can't see them, but inside are typically multiple CPU cores and some kind of cache memory. Random access memory is another type of memory that's attached directly to the motherboard, and it's typically in the form of removable modules.

Connected to the motherboard via cables, we find a hard disk, optical drive, perhaps a solid state drive. The central processing unit is the part of the computer that actually executes instructions, has 1 or more execution cores. Most modern CPUs have at least 2 cores. CPU registers are used for direct computation, direct operations that are occurring right now. The cache is used to store program instructions and data that will be used immediately, the next few instructions that are going to be executed.

This cache is divided into levels of different speeds. It's also volatile. It loses its contents whenever power is disconnected, and it's expensive. This type of fast memory is quite expensive. Random access memory is separate from the CPU.

This is used to store programs and data while the program is in a state of execution. This memory is order of magnitude several orders of magnitude faster than hard disks or even solid state drives, which is why programs are loaded into random access memory by the loader before they start executing. Most programs also buffer their data in RAM for speed. However, the memory in RAM is still volatile. It contents are lost whenever power is disconnected, and it is more expensive per unit storage than disk space or even solid state drive space.

Persistent storage is the slowest of all types of memory in the system. This is non volatile storage for data to be saved whenever the system is powered off. This is the least expensive type of storage per unit of capacity, but it is slow. It's not fast enough to execute programs directly, largely because the CPU would constantly be waiting on the drive to give it the next instruction, and that would slow down the CPU. Hard disk drives are what we typically think of when we think of slow persistent storage.

These are mechanical devices that use magnetic storage, basically a a magnetic medium on top of either a glass or metal disc called a platter, and multiple platters that are stacked spin at high speed with the heads moving back and forth between tracks on the platters. Accessing data on a hard drive is thus subject to 2 types of delay. Seek time, which is the time required to move the heads back and forth, and rotational delay, which is the time required for the platter to rotate around to the location the desired data is stored. Hard drives are relatively inexpensive per unit of storage space, but they are mechanical, which means they're guaranteed to fail sooner or later. Here's a diagram of a hard disk, and here we can see the platters, which are stacked on top of each other and spun around by a motor.

Motor spins us around at high speeds. Head moves back and forth. Now there's typically one head first side of each bladder, so the heads are actually stacked up, and a servo motor moves the heads back and forth. The time it takes to move the head from the inside of the platter to the outside of the platter or from the outside back to the inside to get the appropriate track on the platter where data is located is called the seek time. The time it takes for the spot where the data is located to rotate around under the head is called the rotational delay.

If I have a piece of data right here about where the mouse pointer is, my drive is rotating clockwise, the head has to seek to the track where that data is going to be located, and I have to wait for that data come all the way around and wind up under the head so that it can be read. This adds to delay in accessing, reading, or writing data from the hard drive. In performance critical situations, we'll typically use solid state drives. These drives have no moving parts. They instead use solid state chips to store data.

Consequently, they have no seek time, no rotational delay, and faster burst transfers, much higher performance than hard drives. However, this performance comes at a cost, and solid state drives themselves will eventually fail. Each block of an SSD can only be written the finite number of times before it wears out. The drive itself extends its own lifetime by reallocating blocks dynamically so that each block is written an average number of times instead of a large number of times. And this process is called wear level.

It extends the life of the device significantly. Given that our persistent storage devices are guaranteed to fail in time, it is critical that system administrators keep backup copies of programs and data so that the system can be restored after a failure. Now the backup media should be stored separately from the Computacenter itself. Best practice is on-site at least 10 miles away in a location that can be protected from damage as well as theft or loss of data. Backup media include optical discs such as reportable CDs, DVDs, or Blu ray.

These optical discs are nice because they're inexpensive. However, they do degrade over time. Scientists are still working on the details of exactly how long, but it is known that data will become unreadable eventually. Flash drives such as USB sticks are also inexpensive, although a bit more expensive than DVD media. They're slow, sometimes slower than DVD media, and they themselves will lose data eventually due to electrical properties inside the flash memory.

Tape drives are considered best enterprise level solution, which, of course, means they have the highest costs, but they have the best reliability on average despite being somewhat slow to access for reading and writing. Cloud storage is another option for many users, and in this model, backups are simply uploaded to a remote server that someone else maintains, typically a backup service provider. Cloud storage is relatively low cost and convenient. However, its speed does depend upon the Internet connection that you're using to send data to or retrieve data from the backup service, and theft of data can be a concern. The operating system implements a common mechanism for allowing applications to access and share hardware.

The applications can make requests from the operating system via system calls. The operating system can deliver hardware events to applications, and this is how we allow our applications to make use of the underlying hardware of the computer system. I'll be discussing disk input output and discussing many of the physical properties that come into play when attempting to schedule a hard disk drive for access by multiple programs. In particular, I'll talk about disk attachment, talk about some of the properties of magnetic disks, discuss disk addressing, discuss partitioning, and introduce solid state drives. I'll begin with disk attachment.

Disks are attached to the motherboard via some kind of cable, and the exact type of cable depends upon the bus in use on the system. There are several different types of bus which are implemented by different chips attached to the motherboard on the different computer systems. One common bus that was widely in use until the early 2000 on consumer grade hardware was the integrated drive electronics or IDE bus. This has been since backronymned Parallel ATA or PATA. It consisted of 40 to 80 ribbon cable connecting to a 40 pin connector to provide 40 simultaneous parallel channels of communication between the motherboard and the hard drive.

Enterprise level systems of that time period typically used SCSI or Small Computer System Interface buses, which consisted of the cabling of 50 to 80 pin connectors between the hard disk and the motherboard. SCSI also defined a standard set of commands, a standard protocol, for interfacing with discs, CDs, and other types of storage devices. This protocol was useful for recordable CD media and DVD ROM media and was implemented on the ATA bus using the SCSI protocol in a system known as a TAPI or ATA Packet Interface. Now in modern times, serial interfaces between the motherboard and the disk have replaced for the most part the parallel interfaces. For ATA style disks we have serial ATA or SATA which replaces the 40 pin connector with a 7 pin connector still uses the same protocol either ATA or TAPI protocol and serial attached SCSI or SAS, which uses the SCSI protocol over a narrower channel consisting of 26 to 32 pins.

Both of these new serial attachment mechanisms support higher bus transfer speeds, enabling theoretically faster devices to be attached to the motherboard. It does not necessarily mean, however, that the disks have gotten that much faster. We still store large amounts of information using magnetic discs. These are metallic or glass platters that are coated in a magnetic surface, and a stack of these platters is rotated at high speed by an electric motor. A stack of heads moves back and forth across the platters, altering the magnetic fields in order to read and write data.

Moving the heads back and forth results in seek time, and waiting for the platter to rotate around to the correct position results in rotational delay. Historically, magnetic media were addressed by geometry. The smallest addressable unit of space on a hard disk is called a sector. This is typically 512 bytes, at least on older disks, though other sizes have been used and newer drives go up to 4 kilobytes. Tracks are circular paths at a constant radius from the center of the disk, and each track is divided into sectors.

One head reads from a single track on a single side of each platter. When you stack multiple heads up using multiple tracks on the various sides of several different platters, the result is what's called a cylinder. And historically, accessing disks required accessing the particular data locations using cylinder head sector or CHS geometry addressing. As disks grew larger and faster, however, this type of addressing scheme became limited. And so logical block addressing was put into use and it's now the standard today.

Logical block addressing or LBA gives each block on a disk its own logical address and leaves it up to the disk firmware to convert the logical addresses into physical locations on the disk. Current standards with logical block addressing on ATA will allow enough space for disks up to a 128 heavy bytes. Operating systems normally implement these 48 bit addresses using 64 bit data structures. Thus, operating systems that support 64 bit disk addressing can generally support hard disks up to 8 zebibytes of data, assuming we're using 512 byte sector sizes. Regardless of the size of the disk, it is convenient to partition the disk into multiple sections so that we can isolate data from each other.

We can isolate the main partition of the operating system from a partition we would use for swapping out pages of virtual memory, for example, and we can isolate that from user data. Also, with early hard drives, it was convenient to isolate partitions so as to minimize seek time by making it such that the head didn't have to move as far in order to access data. Now there are 2 types of partition table or data structure that resides on the disk to indicate where on the disk the different partitions lie. A common partition table type that's in widespread use today is the master boot record based partitioning scheme. And the way this works is that the BIOS on the system, the basic input output system, actually loads the first 512 byte sector from the boot drive at boot time and code stored in that 512 byte sector loads the rest of the system.

Also within that sector is stored the partition table, which is called a DOS style partition table, And the DOS partition table still uses legacy cylinder head sector addressing, supports a maximum of 4 primary partitions, one of which can be an extended partition with logical drives in it, and supports maximum partition sizes and maximum partition starting addresses at 2 tebibytes. This is the default partitioning scheme for Microsoft Windows and most Linux distributions. However, as hard drives become larger and grow past 2 terabytes, the GUID partition table or GPT starts to be used. This is a larger partition table that can support disks or partitions up to 8 Zebibytes in size. For compatibility with old partitioning tools and prevent old tools from overwriting sections of the disk and seeing it as free space, A protective or dummy master boot record is retained at the beginning of the disk.

GPT is the default partitioning scheme in Mac OS 10, is an optional partitioning scheme in Linux, uh, and is supported in 64 bit versions of Windows 7, Windows Vista, and Windows Server 2 1,008, provided that the system uses the extensible firmware interface or EFI instead of the legacy BIOS interface. For Linux, the GRUB 2 bootloader can use a legacy BIOS interface, but it requires a dedicated small partition on the hard drive in which to store the rest of the bootloader. Hard drives and magnetic media are used for large quantities of space because of their relative low cost. When high performance is required, we prefer to use solid state drives or SSDs. These drives have no moving parts, which makes them generally faster and less subject to physical damage than mechanical hard disks.

Most of these SSDs use NAND flash memory to store data, and this is a storage mechanism that's based on injecting or removing an electron from a flash cell. Injecting an electron into a flash cell changes its state from 1 to 0, so this is backwards from what one would expect. An empty flash cell actually has a state of 1 instead of 0. The membranes through which this eject, this electron is injected and removed eventually wear out, typically after anywhere from a 100000 to a 1000000 cycles. Furthermore, the electrons tend to leak out over long time periods, periods of of many years, causing flash to lose data, which makes flash based memory systems unsuitable for long term backups.

In this diagram, we can see how a flash or how a solid state drive using flash memory works. Blank flash memory stores the value 111, 111, 111, 111. So we have all ones here. If I want to write the value 1001-0100, I have to pop electrons into the second, 3rd, 5th, 7th, and 8th flash locations. The 8 bit those bit locations.

I'm I'm pretending we only have a byte here. If later I wish to change that stored value to 1010 0011, I must first erase that block of flash memory before I can program the new data value. Generally, I must erase flash memory in the size of an erase block, and this is typically 4 kilobytes. Waiting for this block erasure procedure causes something called write amplification, where successive writes to an SSD become progressively slower. To avoid this problem, typically erase the SSD ahead of time whenever space has been freed on it, and there's an ATA command called TRIM that facilitates this process.

One other issue that has to be dealt with with SSDs is the fact that each cell can only be written to and read from or generally written to, erased and written to, a fixed number of times. This is typically between a 100,000 a 1000000. So to spread out the rights across the entire SSD, the SSD moves data around the drive as files are updated, and it also reserves a certain amount of free space unused so that that free space can be swapped in and out the space that's in use later. This process called where leveling has the advantage of dramatically increasing the useful life of the SSD and reducing write amplification whenever clean blocks are made available. However, in order for the right amplification reduction to be effective, the operating system and the underlying file system must support the ATA trim command, and furthermore, it's impossible to ensure that the disk is secure against forensic data recovery because it may not be possible to overwrite and properly erase the reserved cells of memory that have been taken out of service.

Thus, a used solid state drive should be physically destroyed instead of attempting to resell it, which can be an issue because a solid state drive has a higher upfront cost. So in summary, disks are attached to the system via some kind of bus. The newer bus styles are SATA and SAS. These modern disks are addressed using logical block addressing. They're partitioned either with DOS partition tables or GUID partition tables.

GPT use will increase as the size of disks becomes larger owing to the limits of the DOS partition table, and solid state drives offer higher performance at a higher initial cost subject to the requirement of wear leveling and subject to not being able to be safely resold due to the inability to forensically secure the data on the drive. In this lecture, I'll be discussing disk scheduling. I'll be introducing the purpose of disk scheduling, talking about some classical and historical disc scheduling algorithms, talking about native command queuing and disk schedulers that are currently in use in the Linux kernel, and then I'll talk a little bit about how IO requests can be efficiently scheduled on solid state drives. The scheduling serves 2 purposes. Disk scheduling serves 2 purposes.

The first of these is to arbitrate disk access among different programs. This ensures that competing programs have access to disk resources and that a single program cannot monopolize the disk resources in such a way as to prevent other programs from accessing the disk. With mechanical hard drives, scheduling algorithms historically have also attempted to improve disk performance by reducing the number of seeks required, moving reducing the number of times the drive head needs to be moved. If the drive head has to be moved too many times, a lot of throughput can be lost from the disk because we're waiting on all the seek times, uh, to occur. The simplest scheduling algorithm would be the first come, first serve algorithm, which is implemented in Linux as a no op scheduler.

This algorithm is extremely straightforward. It simply consists of a FIFO queue into which new requests are added. Your requests are removed from the queue in order 1 by 1. These requests are sent to the disk for processing. Now there's no reordering of the queue.

This is a first come, 1st serve ordering. So back to back requests for different parts of the disk may cause the drive head to move back and forth across the platters, wasting quite a bit of time with SEEKs. Historically, several attempts have been made to try to improve this behavior. One example would be the scan algorithm or the elevator algorithm. And in this algorithm, the drive head only moves in one direction.

It serves all the requests in that direction before moving back in the other direction. This is called the elevator algorithm because it's modeled after how an elevator works in a building. The elevator leaves the ground floor, moves to the highest floor, stopping along the way to add passengers traveling up, remove passengers at whichever floor they wish to stop on. And then once the algorithm reaches the or once the elevator reaches the highest floor, turns around and comes back down. Same process here.

In this example with the scan algorithm, assuming that the head starts at sector 1 and this request for sector 50 comes in while sector 61 is being processed, the algorithm is going to process the requests in order 3123240426180497. And since that request for sector 50 came in while 61 was being processed, that request for sector 50 is going to have to wait until the head changes direction and returns to sector 50. Now there are a few optimizations of the simple scan algorithm. The original algorithm proposed that the head would move all the way from the beginning of the disc to the end of the disc and then all the way back to the beginning. The look algorithm improves upon this behavior by moving the head only as far as the highest number of requests before changing directions and moving it back down.

The circular versions of the algorithm, c scan and c look, only serve requests moving in one direction. So, for example, with circular scan, the head would start at the first sector, move all the way to the end of the disk, servicing requests, and then come all the way back down to the 1st sector without servicing any requests and start the process over again. These are historical algorithms in the sense that with modern drives, we have LBA or logical block addressing, and so we don't actually know where the disk is placing data physically. This type of algorithm was used historically with cylinder head sector addressing where we knew the physical properties of the disc, and the idea here was to reduce average seek time. At no time did we know where the platter was.

That was up to the disc to figure out. So there was no way to reduce rotational delay, only minimize seek time. Another algorithm that attempts to minimize seek time is the shortest seek time first algorithm or SSTF. This algorithm actually orders requests by sector location. So when the request for sector 50 comes in, it's kept in an ordered queue, a priority queue, and the next request to be served by the disk, these requests are still sent out 1 at a time, is chosen by looking at whatever request is closest to the correct disk position.

Historically, this could result in starvation because if a bunch of requests come in for 1 piece of the disk, requests for the remainder of the disk might not be processed for lengthy periods of time. Furthermore, once again with logical block addressing, the operating system doesn't really know where the sectors are laid out on disk, and so this algorithm doesn't really work with modern hard drives. Also, with solid state drives, this algorithm assumes there's a disk head, which in the case of a non mechanical drive, there is not. In the Linux kernel, an approximation to shortest seek time first was implemented with the anticipatory scheduler. This was the default scheduler from 2.6.0 to 2.6.17.

It was removed in 2.6.33 because it's obsolete. The idea behind the anticipatory scheduler was to approximate shorter seek time first by ordering only the read requests into an ordered queue into a priority queue. If the next read request was close to the current head position, that request would be dispatched immediately. Otherwise, the scheduler would actually wait a few milliseconds to see if another request arrives for nearby location. And starvation was avoided in this algorithm by placing expiration times on each request and adding preemption so that if a request was waiting too long, it would go ahead and be serviced regardless of its location.

The idea here was to reduce overall seeking. There was a separate queue for write requests because write requests could be performed asynchronously. We did not have to wait on those requests to be completed before a process could continue doing useful work. This algorithm was shown with low performance drives to improve performance on web server applications. However, it was shown to have poor performance for database loads where there were a lot of random reads and writes on the disk.

And with high performance disks, this algorithm actually breaks down. Modern hard disks generally do qualify as high performance disks. The reason being is that they implement something called native command queuing. This is a feature of newer SATA drives, and basically, native command queuing leaves scheduling of disk requests up to the disk itself. The disk circuitry and firmware makes the decision about which request to handle next.

To do this, the disk has a built in priority queue of about 32 entries, and the disk is able to schedule its requests automatically, taking into account both the seek time and the rotational delay, since the disk knows the location of the platter. This makes modern disks much more efficient, and this works with logical block addressing. The operating system's role with this type of hard disk is really more arbitration of the disk resources among the different programs running on the system. One modern Linux scheduler that can be used for such arbitration is called the deadline scheduler. And in this scheduler, the kernel maintains separate request queues for both read requests and write requests, similar to the anticipatory scheduler.

Reads are prioritized over writes because processes typically block or stop and wait while waiting to read something from the disk. Thus, writes can be done later at some point when it's convenient for the operating system. The waiting time in each queue with the deadline scheduler is used to determine which quest will be scheduled next. A 500 millisecond request time is the goal for read request. This is the time it would take to start the request with a 5 second goal to start a write request.

This scheduler may improve system responsiveness during periods of heavy disk IO at the expense of data throughput since each request has a deadline and the longer request has been waiting, the sooner it will be scheduled. This is especially useful for database workloads because there are many requests for different parts of the disk. However, for web servers and other services that try to access large quantities of data located in the same location in the disk, this particular scheduler can actually reduce total throughput. Completely fair queuing is a somewhat different idea where instead of actually scheduling the IO requests, the Completely Fair Queuing model schedules processes or running programs to have time slice access to each disk. And essentially, this CFQ scheduler gives each process an IO time slice, and that process can do as much IO on the disk as it would like within that time slice.

When the time slice expires, the CF key scheduler moves on to the next process and gives it access time to the disk. There is a little bit of similarity here to anticipatory scheduling since a process can send another request during its time slice and try to get that request in without having a lot of seek time. However, this algorithm can waste time if a process does not immediately turn around and send another request. Thus, it can reduce the overall disk throughput since there could be idle times while waiting to see if another request will come in before a time slice expires. This has been the default scheduler in Linux since 2.6.18.

For solid state disks, we have to choose a scheduler that accounts for the fact that there's no seek time to worry about on the disk. The anticipatory scheduler, the elevator algorithms, short of seek time first, All of these algorithms can actually reduce performance on a solid state drive because they make assumptions about minimizing head seek time, and we have no heads to move with a solid state disk. Completely fair queuing can also reduce disk performance because of idling at the end of a time slice. That's true for any disk. So what do we do to maximize performance for a solid state drive?

There are really 2 good choices for a solid state drive. There is the NOAUP or FIFO scheduler. This works well for general purpose systems. However, when there are handy IO workloads and we want to maintain system responsiveness, the deadline algorithm is useful since other processes will have more opportunities to access the disk. In summary, operating systems are arbitrating disk access among different processes to prevent one process from monopolizing the disk and preventing other processes from having access.

On older disks with cylinder head sector addressing, the operating system was also attempting to reduce seek time, thus improving aggregate disk performance. However, newer SATA disks with native command queuing schedule themselves to reduce both seek time and rotational delay. Thus, algorithms that attempt to minimize seek time are unnecessary. Furthermore, with solid state drives, scheduling mechanisms that are based on old mechanical assumptions can actually reduce performance. So for SSDs, we're really only interested in arbitration.

This lecture, I'm going to discuss development cycles. In particular, we'll talk about software life cycle, steps in software development, talk about some software development models, and give a brief overview of formal methods. A software life cycle is a way of expressing how software comes into being and evolves over a lengthy period of time. There's more to software development than simply programming. In particular, we need to plan the piece of software that we're going to develop when we're talking about a new piece of software, figure out what the requirements of that software is going to be, design that software to satisfy those requirements, and then perform several rounds of coding and testing to ensure that our finished application actually meets the requirements before we can put the application into production.

The life of a software application is not over, however, simply because it enters production. Over time, invariably, we will find the need to upgrade the software, which means going through the same development process all over again. And since any software application less trivial than hello world contains bugs, we will have quite a bit of ongoing maintenance in order to find and fix errors that creep up in the software. We'll also need to support users and go through several of these processes before the software finally gets to the point where it has reached its end of life and is replaced by a new application. Software development activities comprise a number of steps.

We have to conceptualize the type of software that we're going to be creating, perform requirements analysis to figure out what the software actually needs to do, come up with a design for the particular application that we wish to implement, and then finally implement and test the application. During the maintenance phase of software life cycle, we need to check and fix bugs. It's important that we be able to replicate bugs that are reported to the from the user so that we're able to verify the bugs do in fact exist and determine the actual location of the problem before trying to implement a fix. Once the fix is implemented, it too needs to be tested before a patch can be released for the software. Now managing these development activities is an important step in and of itself.

Each step of this process requires some kind of orchestration, particularly as the size of the development team increases. A disciplined process helps to maximize effectiveness when the work is being done by more than one person, and the details of this process management are what we call a development model. The simplest development model is the code and fix model. This is really the simplest of models because it exhibits a complete lack of any kind of design discipline. What happens is is programmers simply sit down, write some code, maybe run a couple tests, fix the code based upon the tests, deliver the result to the customer.

If the customer likes the code, then they're done. If not, then go back, fix the code some more, deliver it again. This process continues until the customer is either satisfied or just completely gives up. This type of model is inefficient. The code is often hastily completed, quite a few bugs.

This type of model is, while fairly well suited to outsourced programming, is not really suitable for long term support of a particular application because the project is treated as something that's to be coded and later forgotten. The opposite extreme would be the waterfall model, which is an attempt to follow a conceptual diagram of processes step by step and perform each step all the way to completion before moving on to the next step. So one would start with a concept, perform requirements analysis until all the requirements were teased out, perform a design step until the complete design was finished, and then start the coding. This model doesn't work well in practice because often issues related to requirements do not emerge until after a few versions of the software have been completed, and the user comes back and fills in the development team on details that weren't previously disclosed during the original requirements analysis phase. Similarly, the design often needs to be changed as the source code evolves.

There is a related model called the v model that tries to establish relationships between these steps. However, these models are generally not considered effective. Iterative and incremental models are the most popular software development models used today. And instead of performing each task to completion like one would try to do in the waterfall model, break the project into pieces and implement 1 piece at a time, test and deliver multiple prototypes during the project development phase, and solicit feedback from the customer for each given prototype. This process begins with a concept, but then goes through requirements analysis, a partial design, a prototype, and testing in this feedback process multiple times.

And over time, the software application from its requirements, through its design, through its code are progressively revised until the application actually satisfies the customer's requirements. Once this occurs, some final testing and integration steps can be performed before the software is finally released and put into production. Now there are multiple iterative and incremental development models. Rapid application development is a model that features short planning phase with a large number of prototypes. Agile development puts a strong emphasis on people and interactions over particular requirements documents and other formalities and focuses on responding to customer feedback and developing in a more feedback centric way.

Extreme programming is a form of agile development with extremely short prototype release cycles and very little documentation other than the application itself. For larger teams, a more formal approach is available with the SPIRAL model, which emphasizes risk analysis at each step in order to evaluate the progress of the project and attempt to prevent cost overruns. There is also a branch of development called formal methods. These are used for life and safety critical software systems and are in fact required by the European Union for avionic software and other safety critical applications. Clean room software engineering is a process in which software is formally verified to ensure that it meets all specifications and is statistically tested and statistically quality controlled to ensure that the application is actually performing the tasks that it's supposed to perform.

This is also an iterative and incremental approach. However, it does require much greater documentation of the testing procedures. So how much formalism is actually required in a software development process? Well, doctor Alastair Coburn came up with a scale and determined that the need for formal development processes increases with the size of the development team, the financial risk of the project, or the safety critical nature of the final system, or any combination of those factors. Furthermore, depending on the regulatory requirements in under which the software is being developed, there may be a greater need for documentation and process verification than in regulatory domains where there is little or no such regulation.

So in summary, software development encompasses much more than simply writing program code. Most of the popular development processes that are in use today are iterative and incremental approaches that emphasize building small prototypes and adding on to those prototypes as the project progresses. However, safety critical systems and projects with extremely large development teams or high risk often will need greater formal procedures in order to coordinate the development process and may require formal in this lecture, I'm going to discuss file systems. I'll be providing an overview of the purpose of file systems, discussing metadata that they store, explaining how we create a file system through a process known as formatting, talk about some issues with file systems, including fragmentation and journaling, briefly discuss some internal layouts used by different file systems, and, finally, talk about mounting and unmounting file systems to make them available to users. A file system is responsible for laying out data on a persistent storage device and ensuring that data can be retrieved reliably.

The file system is an abstraction of disk space. It provides routines for querying, opening, and closing files, and providing human readable names for files and some kind of organizational structure for files, typically through directories or folders. Without a file system, we would have to access disk by physical location or by address, and each program would have to reserve certain address limits for its exclusive use. File systems, in addition to performing this abstraction, also arbitrate disk space among different programs and different users of a computer system. File permissions allow users to have a certain degree of privacy, uh, while file quotas ensure that one user does not monopolize the entire system by utilizing all the disk space.

File systems are also responsible for storing metadata or information about each file. This includes a file name, file size, who owns the file, what group that owner belongs to, what permissions exist for each different category of users on the system to access that file, as well as to provide certain timestamps. On UNIX, we have the inode creation time, the file modification time, and, optionally, the last file access time. Metadata records also include internal information that is important for the file system itself, such as pointers to the actual data on disk and a reference count for how many different names refer to the same file, system called hard links. We create a file system by taking an empty partition, or a partition that we're ready to reuse, and formatting it.

Formatting, quite simply, is the process of making a new file system on an existing partition. Formatting typically destroys the structure of any file system that was previously installed on the partition. Thus, when you format a partition, you lose the access to its contents at least through standard tools. However, unless that free space, unless that partition is securely erased, the contents that were formerly on the partition can still be recovered using forensic tools. The only safe way to switch between file systems on a single partition is to back the data that's on that partition up to another disk, format the partition using whatever the new file system would be, and then restoring the data from the backup.

There is no safe way to change a file system type in place without losing data. The file systems do suffer from a few issues. Over time, sections of a file in a file system can become noncontiguous. In other words, a file gets split over different parts of the disk. And in the process, that also splits up the free space so that when new files need to be allocated, they have to be split up to take advantage of smaller blocks of free space.

Uh, this is a situation called fragmentation. It's worse in some file systems than others. Fragmented file systems were a big problem with mechanical hard drives simply because a fragmented file requires a seek to move from the location of the first fragment to the location of the next fragment and possibly further seeks if there are more fragments. Not so much a problem on solid state drives, however, since there's no seek time. And file systems can get around this problem either with offline defragmentation tools that the system administrator can run manually, or they can use fragmentation avoidance strategies or automatic on the fly defragmentation.

Another issue that can occur with file systems is that a single high level file system operation typically requires several low level steps in order to complete. If the computer should crash or power be lost in the middle of those steps being performed, the file system could be left in an inconsistent state. A solution to this problem is called journaling, and the way this works is by recording all the steps that are to be taken in something called the journal, a special part of disk space reserved for this particular information, records all the steps that are going to be taken prior to taking the steps. Thus, if the system should crash in the middle of a file system high level operation, all that needs to occur is the journal simply needs to be replayed next time the file system is mounted, and the steps can be carried out again and leave the file system in a consistent state. Internally, file systems may use one of several different approaches for storing data on the disk.

A simple layout is called the file allocation table, which simply has a single table on each partition to store metadata and the addresses of data segments for each file. File allocation table type storage methods are limited only in terms of how big the file allocation table can be, and these limitations specify, among other things, the maximum size a file can be and how many files can be on the system. Another approach is to use something called inodes, which are data structures on UNIX systems that contain the metadata for a file, including pointers to the actual data. Inodes do not store the filenames, however. These are stored in a separate structure called a directory that maps filenames to inodes.

The maximum number of files that a single file system can hold is limited by the number of inodes created when the file system is formatted. This can be a particular problem for file systems that wind up storing a really large number of very small files. There could be plenty of space left on the file system on the partition. However, if the number of inodes is exhausted, then no more files will be able to be created. Another approach to storing files and laying out data on a file system is through the use of extents.

Extents support larger maximum file sizes because they're designed to allow files to be composed of several non contiguous blocks of space, much like fragmenting that could occur with the file system that doesn't use extents. Extent information is stored with the metadata in the file inode or some other type of data structure for file systems that support extents. Now regardless of how the file system lays out its data internally, we need to make the file system available to users and programs on the computer. To do this, we perform a process called mounting. Mounting is the act of making a file system available to the users of the system.

The opposite process is called unmounting, which is disconnecting a previously mounted file system. Unix systems mount file systems at mount points, which are simply directories somewhere within the overall directory structure of the system. So if I plug in a flash drive on a Linux machine, for example, that flash drive may be mounted at slash media slash my drive. I can mount a large number of different file systems this way at the same time, and I can make all of the different file systems appear to be part of 1 directory structure. On the other hand, Windows systems use drive letters where the letter c is reserved for the system partition, the one on which Windows is installed, and a and b are reserved for floppy drives.

Windows supports a maximum of 26 file systems to be mounted at once simply because that's the number of letters that are available to assign the drives or to assign the partitions. So in summary, file systems organize data, store metadata, provide an abstraction of the underlying storage medium, and arbitrate access to the storage space. We create file systems through a process known as formatting. We can make file systems more robust against data loss during a power failure through the use of journaling. Internally, file systems use various different mechanisms for laying the data out on disk, but no matter how they work internally, we can make them available to a running system by mounting them.

In this lecture, I'm going to discuss requirements analysis. In particular, I'll give an overview of different types of projects, different types of resources that can be applied to projects, talk about the purpose of performing requirements analysis, discuss stakeholders, interests, and actors, and talk about the functional and nonfunctional requirements of development projects. Software development projects can be described as falling into a range in between two extremes. At one extreme, we have something called a greenfield project, and we call it a greenfield project because it's analogous to building a building on an empty field. Start with a grassy field, build everything from scratch.

It's a completely new system, get to make all the design decisions from the ground up. There's no existing code that has to be merged into the system, and this type of project has the potential to be extremely rewarding to the people who work on it. At the opposite extreme, we have a reengineering project or a project that needs to be reengineered. This type of project begins with a pile of existing code that lacks any type of documentation whatsoever. The design of the system, the architecture of the system is an unknown.

And in fact, it's necessary to perform code analysis to determine how the application was designed and how it was architected. This type of project has the greatest potential for frustration since many older programming paradigms did not emphasize making code that was easy to untangle. Most projects fall in between these two extremes. There are some projects that fall into the greenfield category. There are some reengineering projects.

However, many projects are gonna land in the middle. There's going to be some amount of code that has to be reused, yet there'll be some amount of documentation, at least hopefully. Regardless of the type of project, we can utilize several different types of resources in the software development activities. Of course, we begin with people. We have a number of people who work with us on a software development project.

We have software architects. We have designers. We have developers. We have programmers. We have others.

One other type of person that I didn't put in the slides but is important is someone called a domain expert. That's the type of person who knows what types of problems the software actually needs to solve and what the business roles are for that class of problem. Of course, we also have money and infrastructure. Infrastructure includes things such as computer equipment, software applications, desktop environments, other computational resources that we can utilize in order to develop a working solution. Of course, given enough money, we can always purchase additional resources in terms of additional infrastructure or hire additional programmers, consultants, others to help with the process.

Now the purpose of performing a requirements analysis is to make effective use of these resources we have available to us. Ideally, we'd like to keep our development staff happy. We would like to spend our money wisely, and we'd like to optimize use of the infrastructure. We have the aim of trying to determine what exactly needs to be developed so that we can build the right product, so that we can actually make something that satisfies the business needs of whoever has commissioned the product. Speaking of folks who have an interest in the product, we can define a few roles and relationships for specific individuals within a software development setting.

One important class of individual is the stakeholder. Stakeholders are people or other entities such as companies or groups that have a vested interest in the system under discussion. It's often called the SUD. Stakeholders include the customer, the development company itself, users of the system, and other direct participants in the system. However, stakeholders may also include indirect participants in the system.

Perhaps another agency or another unit of the company is actually funding the development work even though they may not directly use the resulting products. These entities have a vested interest in the outcome of the system because they have an investment in the system. Now the interests that one could associate with stakeholders are simply the expectations that expectations that a stakeholder has of the system under discussion. These interests may be of a technical nature. They could also be of a financial nature or even a political nature.

Perhaps there's some aspect of the system that has some political significance to someone. So interests fall into a variety of categories. Actors in a system are anything that has behavior. So this includes both human users and other computer systems. The primary actor in a system is a stakeholder that is using the system directly to carry out some task.

In other words, this is the user who sits down at the keyboard and actually runs the application in order to perform some business function. Supporting actors, on the other hand, are other entities upon which the system relies to complete a task. This could include something such as an external database system, an external printing system, some other functionality that can be described as having behaviors which the system requires in order to perform its business functions. This could even be a person who has to go and manually input some type of data into the system. Human actors typically require a user interface or UI in order to interact the system.

Computerized actors, on the other hand, typically require something called an application programming interface or API in order to interact with the system. Now when we're specifying requirements and considering what type of actions an actor can take with the system, we can divide the requirements into 2 sets. The first set of requirements is called functional requirements. These requirements specify what a software application does. In other words, given some set of inputs, this set of requirements should be able to tell you what the outputs and side effects of the system will be for that set of inputs.

This is the design of the software. This is the low level detail about how the software responds to particular different types of input. Nonfunctional requirements, on the other hand, specify what the software is. These are often called qualities of the software. An example of this is the so called IRPS list, which includes usability, reliability, performance, and supportability.

ISO or IEC 9126 also define some other qualities that a software system may have, including portability and efficiency. Nonfunctional requirements are embodied in the architecture of the software. How is the software put together to meet these quality requirements? So in summary, the types of development projects that we could be considering vary from the extremes of greenfield to reengineering projects. These projects are supported by resources, including people, money, and infrastructure.

Each project has stakeholders and each stakeholder has interest in the system. Stakeholders who use the system directly become primary actors on the system. And software engineering requirements can be classified as functional, in other words, describing how the system works, what the system does, or nonfunctional, describing qualities of the system or what the system is. In this lecture, I'm going to discuss features of the central processing unit or CPU that are useful for supporting multiple applications sharing a computer system simultaneously. In particular, I'll introduce multi programming and discuss the hardware requirements to support multi programming.

I'll discuss CPU privilege modes, x 86 protection rings, mode switches, and briefly introduce interrupts. The first concept to introduce is multi programming, and multi programming is simply the idea that we can run multiple processes or multiple instances of potentially several programs at the same time. And we can do this by having the CPU switch quickly among the different processes, enabling all of them to make forward progress per unit of human perceivable time. The CPU will switch quickly enough to provide the illusion that all the processes are running at same time even if we only have 1 processor core. In the vast majority of modern computing systems, with the exception of some special purpose systems are multiprogramming system.

Some of the old computer systems of the day were batch systems that only ran 1 sis one application at a time. Now in order to support multi programming, we have to have certain features of our computer hardware. First of these is an interrupt mechanism for enabling preemption of running processes whenever some kind of event occurs. We have to have a way of stopping a process, handling an event, and then restarting the process. We need to have a clock so that we know how long a process has been running, and we need to have CPU protection levels to restrict access to certain instructions to prevent processes from hijacking the system.

We're just trying to bypass the operating system altogether. We also need to restrict access to memory in order to prevent reading and writing to memory that the particular process does not own, belongs to somebody else. 2 CPU protection levels are sufficient, a protected mode and a privileged mode. These modes are also called the supervisor mode or kernel mode and user mode. In kernel mode, all instructions on the CPU are enabled, and the kernel can access all memory on the system.

In user mode, the CPU disables all the privileged instructions and restricts most direct memory operations. Thus, a user program must make a system call to the operating system to request memory or perform other resource allocation tasks. In this way, the user processes are effectively sandboxed both from the system and from each other. On the Intel based systems, x86 and x8664 systems, there are actually 4 modes available. These are implemented by what are known as x86 protection rings, which consist of 4 privilege levels numbered 0 through 3.

Ring 0 has the greatest number of privileges. Code executing in ring 0 can execute any instruction the CPU provides and can access all memory. Ring 3 has the fewest privileges. All the instructions are restricted to the set of instructions that are relatively safe. In practice, most operating systems actually only use ring 0 and 3.

OS 2 and Zen are the notable exceptions that make use of ring 1. Newer systems with virtualization extensions, either the Intel VTX Extensions or the AMDv extensions, add an extra privilege level below ring 0. This is colloquially sometimes referred to as ring negative 1. And this mode enables instructions that allow multiple operating systems to share the same processor. These instructions help system support hosting multiple virtual machines at the same time.

Regardless of the number of modes available, whenever we wish to change modes for whatever reason, we have to perform something called a mode switch, and that occurs whenever the CPU switches into user mode, kernel mode, or hypervisor mode, or whenever an x86 CPU changes which protection ring is presently effective. Mode switches have the potential to be slow operations compared to other machine instructions depending upon the hardware. A notable example was the 1st generation of Intel Core 2 series processors in which the mode switches into and out of hypervisor mode were quite slow. One situation in which a mode switch might occur is when something called an interrupt happens, And an interrupt is simply a situation in which the currently executing code is interrupted so that an event can be handled by the operating system. Interrupts can fall into 2 categories.

We can have involuntary interrupts, which are external to running processes. These consist of things such as IO interrupts, which are generated every time you press a key on the keyboard or perform any other IO task, clock interrupts, which are timer mechanisms that can be scheduled to go off at a particular time and page faults, which have to do with the virtual memory subsystem. Interrupts can also be voluntary. In other words, created by a process that's running. System calls and exceptions such as said default or divided by 0 actions can result in interrupts as well.

And the CPU provides hardware mechanisms for detecting when an interrupt is occurring and handling the interrupt. So in summary, multi programming systems allow multiple applications to run simultaneously. Implementing multiprogramming reports requires support from the hardware. In particular, we need CPU privileges, we need a clock, and we need some kind of interrupt handling mechanism. The CPUs used in multi programming systems need to have at least 2 privilege modes.

Intel x86 systems support 4 or 5 modes depending on the processor. Mode switches can be expensive in terms of performance, so we don't want to do them more than necessary. And interrupts enable hardware events to be delivered to applications, and they allow applications to yield control of the system while waiting for events or waiting for service. In this lecture, I'm going to discuss kernel architectures. I'll begin by introducing the functions of the kernel, explain the separation between mechanism and policy, talk about some seminal early kernels in the history of computing, and then introduce the differences between monolithic kernels and microkernels.

A kernel provides 2 functions, the same two functions as in the operating system. It provides abstraction and arbitration. The kernel provides abstraction in the sense that it provides a mechanism for programs to access hardware, a way to schedule multiple programs on the system, and it provides some method for interprocess communication, or IPC, a way for programs to send messages to each other or send messages to hardware devices, or out to the network. Kernels also provide abstraction mechanisms. They ensure that a single process or running program can't take over the entire system.

They enforce any kind of security requirements such as access privileges that might be in place on the system, and they minimize the risk of a total system crash from a buggy application or device driver. It's important to distinguish between mechanism and policy when discussing the internal components of an operating system. The mechanism, put simply, is the software methods that enable operations to be carried out. An example of a mechanism would be code that, implemented inside a device driver, sends a message to a device that causes that device to blink a light, enable a camera, or perform some other hardware level operation. Policy, on the other hand, is a set of software methods that enforce permissions, access rules, or other limits against applications.

So a policy, for example, would be something that said that only users who met certain criteria could send a message out to a hardware device to blink a light or enable a camera or perform some other hardware function. It's a generally accepted principle of good design The mechanism and policy should be separated as much as possible. An early kernel that separated mechanism and policy quite well was the regnissentraland RC 4000 monitor kernel. This kernel was developed in 19 69 primarily by Per Brink Hansen for the Regenstentraland RC 4000 computer system. This was a computer system that was developed in Denmark, and the central component of the system was a small nucleus, as Brink Hansen called it, called monitor, which allowed programs to send messages to each other and allowed programs to send and receive buffers, which were essentially types of messages for hardware, to and from different hardware devices.

In particular, at that time, they had a card reader a tape reader, and a printing style output device. Other kernels with different scheduling mechanisms and other capabilities could be run under monitor. In those days, it was not clear that multi programming was really a desirable feature for computing. Thus, someone could write a multi programming capable kernel and actually run that as a sub kernel under the monitor system. Importantly, this was also the first system that allowed sub kernels and systems level software to be written in a high level language, in this case, Pascal.

The system performance was actually quite awful. Brink Hansen stated that the operating system itself was so slow at performing its IPC tasks that there were a number of issues with the system completing tasks on time. However, the system was stable and reliable, making it successful in computer science history even if it was not a successful product commercially. On the other hand, the opposite extreme would be the UNIX kernel. This was developed at Bell Labs by a team headed by Ken Thompson and Dennis Ritchie, also starting in the late 19 sixties.

The difference between the UNIX kernel and the r c 4,000 monitor was that the Unix kernel's design implemented performance. Thus, instead of having a very small kernel that simply provided an IPC mechanism and some basic resource collision avoidance, this kernel actually provided all the device drivers, all the scheduling, all the memory management including support for multi programming directly inside the kernel. This kernel was an early example of what would later be called a monolithic kernel. A monolithic kernel is a kernel that contains the entire operating system in kernel space, runs all of the operating system code in privileged mode or ring 0 on an x86 system and divides the different functions of the operating system into subsystems of the kernel. All of these subsystems, however, are run-in the same memory space.

This has the advantage of higher performance, but the disadvantage is that the kernel becomes less modular and more difficult to maintain, and the components are not separated very well. So a crash in one component could in fact bring down the entire system. The opposite of this, the r c 4 1000 style kernel, is what we now call a microkernel. And a microkernel basically contains the bare minimum of code is necessary in order to implement basic addressing, inter process communications, and scheduling. This basic amount of code runs internal space and everything else runs in user space, often with lower privileges.

As a general rule of thumb, microkernels contain less than 10,000 lines of code. Micro kernel based operating systems tend to be quite modular because they divide the operating system functions between the kernel and a set of servers that run-in userspace. However, because many of the core functions of the operating system are performed by userspace components, which have to communicate with each other via the kernel, performance does suffer. Thus, most kernels that are in use today are a hybrid of these two designs. I'm going to introduce Murphy's Law of Reality, sort of an extension of the Murphy's Laws with which you may be familiar.

And my definition of Murphy's law of reality is simply that reality is the hazy space between the extremes of competing academic theories in which everything is wrong in some way, at least according to the theories. This idea of a hybrid kernel architecture is a controversial one. Some people do not like to use this terminology at all. Many people prefer to keep the binary classification of monolithic kernel and microkernel. However, if we look at modern kernels, typically the monolithic versions of modern kernels are broken into modules that can be loaded and unloaded at runtime.

This helps to increase maintainability of the kernel. And true microkernels today would have unacceptable performance. Thus, microkernel based systems typically have some of the features of monolithic kernels such as more device drivers and other code that runs inside the kernel's memory space. Some examples of different types of kernels for monolithic kernels, in addition to the system 5 UNIX kernel, which is a descendant from the original UNIX kernel. We have the Linux kernel, BSD, MS DOS, and Windows 9 X kernels.

Windows NT, XP, Vista, and 7, if you don't prefer to use the hybrid terminology, would also qualify as monolithic kernels. And the Mac OS 10 kernel falls into the same category. In terms of microkernels, the r c 4,000 monitor kernel would have been the earliest. However, there have been plenty other examples, including Mach, L4, the MIT exokernel project, and the idea at least behind the Windows NT kernel, which was based upon a microkernel design. The same is true of the Mac OS 10 kernel since that was originally based on the Mach microkernel.

However, those have been heavily modified, and now I have many properties of monolithic kernels also. So in summary, the kernel is the minimum layer of software inside the operating system that provides the basic foundations for abstracting away details of the hardware and arbitrating between multiple applications. When the bare absolute bare minimum implementations are used, we call the result a micro kernel. Monolithic kernels on the other hand have all their major OS components contained within them, running everything inside kernel space to improve performance. 2 early influential kernels were the RC 4,000 monitor, an example of a micro kernel, and the original Unix kernel, which in practice, however, most modern operating system kernels are hybrids of the two designs and have features of both kernel type.

In this lecture, I'm going to introduce the Unified Modeling Language, UML. I'll be discussing software modeling introduce UML talk about some of its limitations discuss a few of its diagram types, discuss the role of UML in software engineering, and mention a few case tools. In software systems, especially complex software systems, it's useful to be able to present a design to other developers for discussion during the development process. Complex software systems need to be designed before they can be constructed or modified anyway and it's helpful to keep design documentation on hand so that the system can be modified more easily later. In order to communicate with other developers and other stakeholders in the system, it's helpful to have some analog to a contractor's blueprint.

Consider, for example, in the real world, building a large building, such as a hospital. First, an architect will come up with a high level design for the project. Engineers will design the individual components and ensure that the hospital will meet all codes and remain standing and operational. And then during construction, the designs get updated slightly to account for changes in any problems that are encountered during the construction phase. After construction, the blueprints are retained in case an addition is needed later or in case modifications are needed to the building.

Unified modeling language provides an option for software engineers to have similar design documentation for software projects. In the early days of software engineering, diagrams were drawn using a wide variety of different formats with different symbols, different connectors, different annotations, and so forth. And developers moving between companies or trying to collaborate across company or even divisional boundaries within a company had to be trained on each specific format was being used for the communication. The idea behind modified modeling language was to provide a single standardized formatting in a standardized set of symbols in order to create a common language. And the group of folks that did this were Grady Booch, Ivor Jacobsen, and Jim Rumbaugh at Rational Software Corporation, which was later purchased by IBM.

UML was standardized by the object management group, but the standards are not always followed by UML tools. And in particular with the latest version of UML, there is no set of tests in order to verify tool compliance with the standards. However, UML has become popular for drawing certain types of diagrams either manually by hand using different tools or using electronic tools. Individuals drawing UML diagrams by hand typically do not remain strictly adherent to the standards. There can be slight variations in the diagrams as drawn.

And UML provides diagram types to model system structure, behavior, and interaction. There are some limitations to UML. First and foremost, UML does not replace other types of design documents. A good example of this is use cases. UML use case diagrams can provide useful indexes into use cases.

However, they do not replace the use cases themselves. Many of the diagram types that UML provides are not actually used widely simply because they're specialty diagrams that many developers don't choose to utilize. UML diagramming tools often use incompatible formats making them difficult to interchange diagrams between one tool and another. And standardization of UML was a controversial and in some circles remains a controversial topic today. Other software modeling approaches do exist but were not included in UML because they were not considered by the UML authors.

Nevertheless, UML does provide a rich variety of diagram types. These can be classified into 2 to 3 categories depending on how you prefer to draw the classification. Classification. We have structural diagrams, in particular class diagrams, that are widely used to show how object oriented systems are designed. We have behavior diagrams, in particular activity diagrams, which can show how a development process can be coordinated, and state machine diagrams, which are useful for representing finite state machines that exist in many types of software.

There are also interaction diagrams, which can be thought of as a subset of behavior diagrams, and this can show how different software components interact with each other. UML as a tool in software engineering is simply one of a number of tools that can be used to plan, architect, and design software applications. Diagrams using UML are convenient for communication between different members of a development team, and they help communicate the architecture and design of an implemented software system to future developers who might need to maintain that system. There are some tools available called case tools or computer aided software engineering tools that can take UML diagrams a bit further and actually generate partial program code from the UML diagram. So if you have a UML diagram laying out a few classes, the case tools can often fill in templates of the classes, and programmers would only need to fill in the code for each individual method in the class.

Other tools exist that can actually take existing code and extract a UML diagram from that code showing how the code is structured. So in summary, UML provides us a common set of communication tools for presenting and discussing aspects of the architecture and design of software systems. UML is one of many tools used by software engineers, and software applications exist for drawing UML diagrams and potentially generating code from those diagrams. However, these tools are often not interoperable with each other. In this lecture, I'm going to discuss UML activity diagrams.

UML activity diagrams are used for modeling processes. These processes can include different operations within software. They can include the software development process itself, can also include business processes, and other types of process. In this example graphic, I have a UML activity diagram illustrating the development for a simple client server project in which the client and the server are implemented by different members of the development team. The basic parts of a UML activity diagram include the initial state, the final state, and set of transitions and other states and controls in between the initial state and the final state.

The initial state is represented by a filled circle and represents the point at which we enter the process. The final state is displayed as a filled circle within another circle, and this is the state in which we exit the process. In between these two states, we have action states and activity states. Action states represent single operations in the process being modeled, and these operations are not broken down into another activity diagram. Activity states represent longer running processes that occur as steps in our outer process, and these inner processes have their own activity diagrams to explain how they function.

Within an activity diagram, we can make a decision to perform one step or another or even repeat a prior step based upon some condition. In a UML activity diagram, this decision is called a sequential branch, and it's represented by a small diamond borrowed from flowcharting symbols. A decision can be taken at the point of the diamond, and then transitions leading from the diamond will indicate the next state. Conditions associated with those transitions are included in square brackets as annotations next to the transition arrow. The special annotation else indicates a path to take if no other condition is satisfied.

In modeling processes, it's useful to show where certain steps can be carried out by different people or different components of the system in parallel. We can represent this in a UML activity diagram by means of something called a concurrent fork. A concurrent fork is simply a thick line to which a transition connects and several transitions originate. From the originating transitions, we have states that are reached in parallel, presumably by different entities within the system. Thus, the concurrent fork allows our process to be split into multiple concurrent paths, things that can be done at the same time, and then we can get back together later.

And the way we come back together from a concurrent fork is with the opposite, a concurrent join. This is the same kind of heavy line. However, transition arrows come into the line, and a single transition comes out of the line to the next state in the sequential process. So we have multiple concurrent parallel paths meeting to form a single sequential path. It's useful with concurrent forks and concurrent joints to have something called activity partitions or swim lanes.

These are ways of annotating the diagram to show which component, individual, or entity is responsible for each parallel path in the project. In this example, we can see that the entire team performs research and then performs a design step. Then we have a concurrent fork, after which Bob implements the client, and Jill implements the server. After those implementations are done, we have a concurrent join and come back to the team. Separation of responsibility is indicated by drawing a box or activity partition around those parts of the path that are the responsibility of 1 single component, 1 single team member, or 1 single entity.

I'll take just a moment to talk about types of tools that we can use to draw UML activity diagrams. A simple way is to draw the diagrams by hand on paper. If we need a digital copy, we can simply use a camera, such as the type of camera in your cell phone, to make a digital image. Some more professional looking results can be obtained by using a drawing program. I've used Google Docs drawing tool to make the examples in these lecture slides.

However, you could use OpenOffice Draw, Visio, or another type of drawing program in order to make these diagrams. There are also some open source tools that have UML specific additions that can be useful for making UML diagrams. These include dia, certain plug ins to the Eclipse framework, however, these are complicated to set up and sometimes difficult to use, and Umbrella, which is a general purpose computer aided software engineering tool. So in summary, UML activity diagrams allow us to model a process. This process can fall into any one of several domains.

This could be a process inside software. This could be a business process. This could even be the software development process itself, how the individual team members are going to come together in the project. In this example, I am modeling a software development process, a very simple one. You can see the initial state, the filled circle at the top at which the development process begins.

This process begins with the entire team performing research. Research is an activity state, so it would be described by a separate activity diagram, which I have not shown. After the research phase, we would move to the design phase in which the entire team would be working on the design for the software. After the design state, which is an action state, so it's not further illustrated anywhere else, we have a concurrent fork at which point Bob goes and implements the client part of the project, and Jill goes and implements the server part of the project. When both Bob and Jill are finished with their implementations, we have a concurrent join, at which point the entire team reassembles to integrate the project and test the project.

If all the tests pass, we move on to the ship state. However, if the tests are not passing, if we take the else branch of that sequential branch, then we go back to the test step. Finally, once we've moved to the ship state, we can move to the final state of the diagram which indicates that this process is complete. In this lecture, I'm going to discuss interrupts and device input output. When hardware devices on a computer produce events, we need some way of being able to handle those events within the operating system and deliver them to applications, and hardware devices are going to produce events at times and in patterns that we don't know about in advance.

For example, we don't know which keys the user is gonna press on the keyboard until the user actually presses those keys. If a cat walks across the keyboard, we're gonna see a completely different pattern of key presses from which we would expect to see with the human user. Similarly, if we have incoming network packets, if we're running a server application or even just a workstation, and we have messages coming in from the network, we don't know the order and timing of those messages. We also don't know when the mouse is going to be moved or when any other of a whole bunch of hardware events is going to occur. So how can we get the information generated by these events and make it available to our applications for use?

Well, we have 2 options. First option is that we can pull each device. We can ask each device if it has any new information and retrieve that information, Or we can let the devices send a signal whenever they have information and have the operating system stop whatever it's doing and pick up that information. This is called an interrupt. The polling model of input involves the OS periodically polling each device for information.

So every so often, the CPU is going to send a message to each hardware device in the system and say, hey, you have any data for me? And most of the time the device is going to send back, no, don't really have any data for you. Other times, the device is gonna send back some data. It's a really simple design, extremely simple to implement, but there are a number of problems with polling. First problem is is that most of the time when you're polling, the devices are not going to have any input data delivered.

Thus, polling is going to waste a whole lot of CPU time. The second issue that occurs is high latency. If I press a key on the keyboard, that keystroke is not going to get transmitted to the computer until the next time the CPU holds the keyboard to ask which keys have been pressed. If that time is set to be really short, I'll have good responsiveness, but the CPU is not gonna get any useful work done. On the other hand, if we set that time length to be long enough for the CPU to get some work done, there's going to be a noticeable lag between the time I press a key and the time a character appears on the screen.

Since the device must wait for a polling interval because it can translate input, we're going to have a high latency situation. And again, shortening that polling interval to try to reduce the latency simply wastes a whole lot of CPU time checking devices that have no input. So a better mechanism is to use a system called interrupts. And with interrupts, the hardware devices actually signal the operating system whenever events occur, or more precisely, they signal the CPU, and then it's up to the operating system to receive and handle that signal. What the operating system will do is it will preempt any running process.

In other words, it will switch what we call context away from that running process to handle the event. Basically, it will move the program counter of the CPU to the code to handle that particular interrupt. This allows for a more responsive system than we could ever achieve through polling without having to waste a whole bunch of time asking idle devices for data. However, this does require a more complex implementation, and that implementation complexity begins at the hardware level. Specifically, within the CPU, we need to have a mechanism for checking and responding to interrupts, and this mechanism is implemented as part of the CPU's fetch execute cycle.

In the process of fetch execute, the CPU is going to fetch an instruction from memory, increment the program counter, execute that instruction. But instead of simply going back to the next fetch, the CPU actually has to have additional hardware to check to see if an interrupt event is pending. If there is an interrupt pending, then the CPU has to be switched to kernel mode if it's running in user mode, so the privilege level needs to be escalated. Save the program counter by pushing it onto the stack, and load a program counter from fixed memory location, and that fixed memory location is called the interrupt vector table or IVT. So we load the program counter from the IVT, and then the CPU goes and executes that new instruction the next time the fetch execute cycle resumes.

So the CPU actually moves from executing program code to executing code from the interrupt handler for the particular event. If no interrupt is pending at the end of an execute, then we simply go back to the next instruction fetch. The interrupt vector table consists of an array of addresses of handlers. Each element in this array essentially gives the program counter location for the handler for a particular interrupt. This handler is going to be in a subsystem of the kernel for a monolithic kernel, or this handler might invoke a call to an external server for a microkernel.

In any case, however, the first handler by convention, element 0 of the array, is always the handler for the clock. Then handlers for different devices are in the array after the clock handler. So, the interrupt vector is always mapped into the user part of memory. It's always available at all times so that the kernel can go and look up interrupt information whenever is necessary. An interrupt is processed by branching the program counter to the interrupt handler, executing interrupt handling code, and then at the end of the interrupt handling code, there'll be an instruction to return from the interrupt.

In the Intel assembly language, this is the IRET instruction, which loads the process program counter back from memory, it pops the stack to get the original program counter back, and goes ahead and changes the CPU back to user mode, so it removes the privilege escalation. The interrupt handling mechanism is thus able to handle events from hardware devices without having to pull each device individually to get data. In this lecture, I'll be discussing interrupt controllers. In particular, I'll introduce the old and new mechanisms for delivering interrupts from hardware devices to the CPU. These methods include the original programmable interrupt controllers and the new advanced programmable interrupt controller with message signal interrupts.

Interrupt controllers provide an interface for hardware to signal the CPU whenever device needs attention. It's important to note that this signal only includes a message that essentially says, hey, I'm a device, I need attention. The CPU historically then actually does have to go and pull the device to get any data that the device may have. The older mechanism performing this operation was called a programmable interrupt controller or PIC, and it actually required dedicated lines to be added to the motherboard. The ISA or industry standard architecture bus, which dates back all the way to the first PC back in 1987, and older versions of the PCI, or peripheral component interconnect, bus utilized this mechanism.

The new mechanism or the advanced programmable interrupt controller is used on PCI Express devices and some newer PCI devices. The old controller, or the programmable interrupt controller, actually consisted of 2 programmable interrupt controller chips that were attached to each other, with one of the chips being attached to the CPU. The so called master chip was the one attached to the CPU, and pin 2 of that master chip was attached to a slave chip. Each pin on each of the 2 chips allows for 16 interrupt numbers to be created. Interrupts 0 through 7 are correspond to the pins of the master chip, and interrupts 8 through 15 correspond to the pins of the slave chip.

Now it should be noted that since pin 2 of the master chip handles the slave chip, that the master programmable interrupt controller only supports an effective 7 interrupts. So there are only 15 usable interrupt hardware lines for devices. And these are numbered 0 through 15, but we have to skip the number 2. Now, historically, pin number 0, which corresponds in software terms to what we call interrupt request line or IRQ0, was connected to the timer. Interrupt request line 1 was connected to the keyboard.

Different ISA and PCI devices could then use the remainder of the master chip by connecting to IRQ lines 3 through 7. On the slave chip, pin 0, which corresponds to IRQ 8, was connected to the real time clock. Pin 4 corresponding to IRQ 12 was connected to a PS2 mouse. Pin 5 or IRQ 13 connected to the math coprocessor, which was a separate component from the main CPU in earlier PCs, and then pins 67 corresponding to IRQ lines 14 and 15 connected to the IDE controllers. These were used for disk and eventually for optical devices.

This left pins 1 through 3 on the slave controller or IRQs 9 through 11 available for hardware devices. Now these interrupt lines on the motherboard were actually circuit traces. These were conductive pads etched into the motherboard that allowed interrupts to be received from devices. There were 15 lines available of the 16 that could be used by devices with lines 0 and 1 reserved for the timer and a PStwo keyboard respectively. Actually, even before the PStwo reservation, the original AT keyboard.

ISA and PCI add in devices actually had to share interrupt request lines, and this sharing could lead to hardware conflicts that could lock up the system. It was thus up to the system owner to manage the sharing by setting little jumpers on the add in cards so that the cards were using different IRQ lines. There were also performance issues when IRQ lines were shared because the operating system actually had to pull each device sharing an IRQ to determine which device it was that raised the interrupt. Pulling was still necessary in order to receive any kind of data from the device regardless of whether it was sharing an interrupt line or not. On modern systems, a completely different interrupt mechanism is used, and this mechanism has a set of memory registers on what's called an advanced programmable interrupt controller.

And this set of memory registers is connected to a single shared bus Each device on the system can use to raise an interrupt message by writing that message into one of the memory registers. These are called message signal interrupts using the MSI and MSI X specifications. Essentially, each device here I have a timer, RTC, USB host controller, SATA controller is attached to the bus and indicates its interest in raising an interrupt to the APIC by sending a message over that bus. Now this message does not contain any data. It's only a request for attention.

If the CPU has to be involved in the operation of sending or receiving information, then the CPU actually has to contact the device, in other words, pull it directly. There is a way around this called direct memory access or DMA transfers, which are used extensively on PCI Express devices. The register on the APIC stores the request for attention until such time as the operating system handles the interrupt request, and then that message is cleared from the APIC. This is the only interrupt mechanism that's available on PCI Express buses. There are no hardware interrupt lines.

However, a number of motherboards still have interrupt lines physical interrupt lines and have physical pick pins so that they can support legacy devices. There are a number of specialty legacy devices still in use that need to be supported. Message signaled interrupts do solve a number of problems with interrupt request sharing. The original specification allows each device to use any one of 32 IRQ lines. The MSI X specification will allow each device to use up to 2,048 virtual lines virtual interrupt request buffers, essentially.

And this allows for less contention and reduces the need to share interrupt request numbers by device, thus reduces the amount of time necessary for the CPU to determine which device wanted attention. So, main thing to take away from this is that the interrupt controller and the interrupt request mechanism only allows a device to raise a signal that says it wants attention. It's up to the CPU or on certain buses up to the device and the memory controller to get the information out of that device and into memory. In this lecture, I will introduce use cases. Use cases are a type of requirements document that provides narrative descriptions of scenarios that will be handled by the system.

I will be discussing the elements that make up a use case. These elements include the trigger, actors in the use case, preconditions, steps in the process described by the use case, guarantees made to the stakeholders, and any quality requirements applicable to the use case. Use cases are one type of requirements document. They provide a written narrative that describes how the system under discussion or SUD will behave in given scenarios. After the system is implemented, the SUD could be tested by following the use cases and checking the behavior of the system matches the requirements for the given scenario.

It is important to remember that a use case is only one type of requirements document. Other types of requirements documents, such as the system conceptualization or user interface specification, do exist. The primary function of a use case is to describe how the SUD responds to interaction from the stakeholder in the system In other words, a use case specifies what the SUD is to do in different scenarios. For example, a use case for a database system might specify what steps are to be taken whenever the database administrator asks to update a record in the table. On the other hand, a use case might specify that certain information needs to be updated whenever a customer makes a new order from a company.

These two scenarios illustrate the different levels at which use cases can be written. At the high level, we have business use cases that describe the business process that must be carried out without specifying any details of the system. These use cases are also sometimes called essential use cases. Lower level use cases, or system use cases, describe how the SUD will behave in specific situations. These use cases incorporate details of the technology being used.

Use cases may have either black box or white box scope. All business use cases and many system use cases use black box scope. This means they treat the system as a mystery device. They specify how the system will behave under given conditions, but they leave out all the details of how the behaviors will be implemented. Some system use cases on the other hand may be written from a white box perspective.

These use cases detail some of the internal operations of the SUD, such as specific protocols or algorithms that will be used. White box use cases are especially useful for describing how different components of the SUD will interoperate with each other. When writing use case narratives, it is customary to include a number of standard pieces of information. The specific entries and the order of the entries will vary by organization. In most cases, you will write use cases by following a template.

Sample use case template might contain items such as the trigger for the use case, a list of actors involved in the use case, and a list of preconditions that must be true before the use case steps can begin. The use case then specifies the system behavior is a series of steps illustrating how the primary actor interacts with the SUD. At the end of the scenario described by the use case, certain guarantees are made to the stakeholders about the state of the system. The use case might also specify quality requirements such as the maximum amount of time allowed to complete the scenario. In general, a use case will have a triggering condition.

This trigger corresponds to the user wishing to perform some action using the SUD. Alternatively, this trigger could be the result of an event that occurs somewhere in the system or outside the system. Preconditions are statements that are assumed to be true when executing the use case. These conditions are not checked anywhere in the use case. Use cases guarantee that certain conditions will be true at the end of a process.

These guarantees are promises made to the stakeholder as to how the system will behave. Ideally, the process will be successful, and the SUD will deliver all guarantees presented by the use case. However, if an error or unexpected condition occurs somewhere during the process, some minimal guarantees about the state of the system should be given. Actors in a use case are external persons, systems, or other entities that interact with the SUD in some way. These actors include the user or users of the system, any entities upon which the system depends in order to carry out a business function, and any other systems with which the SUD interfaces.

The actor that performs the actions in the use case is called the primary actor. In a general sense, the primary actor is the user of the system that will be created. Thus, by definition, the primary actor is also a stakeholder in the system. Supporting actors have roles in carrying out some operation of the system, but they do not utilize the system directly. In a business level use case, the supporting actors have important roles in the business process, but they do not carry out the process directly.

Now use case documents are narrative texts. They tell a story about how the system is going to act under given scenarios. In order to communicate the story effectively, there are certain rules of thumb that should be followed when writing use cases. 1st, use simple language with active voice and simple sentence constructs. For example, say, the user sits down at the keyboard, instead of, keyboard is presented to the user.

2nd, do not include user interface elements in the use case. Say, the user enters data into the system instead of the user types characters into a text box in the middle of the screen. Leave the user interface design to separate specifications or prototypes. Finally, every use case should end with least one guarantee about the state of the system. If the process is successful, how does the system react?

If the process fails, what is the condition of the system after the failure? Remember that computer software can and does periodically fail. Also, remember that human users are guaranteed to fail and that the system should be made robust or as idiot proof as possible. Now you may be aware that the Unified Modeling Language provides a graphical use case diagram. You may also believe that these diagrams will save time compared to writing use case narratives.

Unfortunately, graphical use case diagrams do not provide much information about the process being described. They illustrate a stick figure representing a primary actor executing a use case, represented by a box containing the use case title. However, the graphical diagrams do not provide any detail about the process. As such, the diagrams are useful for providing indexes into the use case narratives, but they do not add any real value to the use cases themselves. A simple textual index with numbered use cases would accomplish the same purpose with less effort.

Agile methods, particularly extreme programming, have extremely short iteration times that are not conducive to formal use cases. One lightweight alternative to use cases is the user story, which is a short and simple way of expressing a particular requirement. User stories are written for each scenario similar to the approach used in use cases. However, instead of a formal document, the user story simply expresses what a primary actor would like to achieve in the system. For example, a user story for a database administrator might read, as a database administrator, I want to be able to perform an operation that updates the name, address, and age fields in a single transaction.

User stories typically omit some design details, but these details can be discovered later as the software is developed. Also, these stories can be either black box or white box in perspective depending upon the needs of the development team. In summary, use cases describe a scenario in which primary actor interacts with the system under discussion. Each use case is named according to the process it describes, and use cases generally have triggers indicating the conditions under which the process is applicable. Use cases may specify preconditions, which are assumed to be true prior to execution of the use case.

Use cases provide an outline of the steps required in the process along with a set of guarantees about the state of the SUD after the scenario is finished. Quality requirements, such as execution time limits, may also be specified. Agile development methods, especially extreme programming, disfavor use cases due to the time involved in creating the documentation. These short iteration development techniques use user stories and other lightweight methods instead. In this lecture, I'm going to discuss interrupt handling.

I'm going to begin by discussing interrupt handling at the hardware level, and then move on to features provided by the CPU, and finally, features of the operating system for handling interrupts. At the hardware level, devices are connected either via traces on the motherboard or via a shared messaging bus to the advanced programmable interrupt controller. The CPU checks for hardware interrupt signals from this controller after each user mode instruction is processed. So after each instruction is executed running some particular program on the system, the CPU actually checks to see if there are any interrupts that need to be processed. If an interrupt signal is present, then a kernel routine is called by the CPU in order to handle this interrupt.

The interrupt dispatch routine, if it's not implemented directly in hardware, is actually a compact and fast routine that could be implemented in the kernel, often coded in assembly language. It has to be that fast. The specific interrupt handler is always a kernel routine, or in the case of a micro kernel, an external server routine. And this specific handler depends upon the type of interrupt received. These are typically coded in C.

So once again, the fetch execute cycle, we check for an interrupt pending after each instruction is executed. If there is an interrupt pending, we escalate privilege to kernel mode, push the program counter onto the stack, in other words, save it so we can resume from that point in whatever program we're interrupting, and then go and handle the interrupt. We do this by loading the program counter, the new program counter that is, from a fixed memory location provided to us by the interrupt vector table. The interrupt vector table gives us the address of all the different interrupt handlers. We need a separate handler for each type of interrupt.

Clock requires different logic from the keyboard, for example. Other devices, such as, say, a webcam attached to your computer, needs different logic in order to process messages from it. So we have different en route handlers for each of these different devices. The table simply stores the addresses of each handler. And in our monolithic kernel case, the handlers are actually part of the kernel and are mapped into kernel memory space.

The interrupt vector table consists of a list of each of these addresses for kernel handlers, and conceptually is mapped into both kernel memory and user memory so that it can be accessed quickly. And, historically, these started at address 0. However, the mappings are different depending on the architecture. On Intel based systems, we have something called the interrupt descriptor table. The IDT provides special instructions and data structures that are actually managed by the CPU itself, so the interrupt handling can be as fast as possible and protection rings can be changed automatically.

The IDT is simply a reserve block of RAM used by the CPU to jump quickly to a specific interrupt handler. This IDT is mapped into kernel space, which was originally beginning at address 0. But this mapping is flexible with modern CPUs and can be mapped into other parts of the memory space. The first 32 entries of the IDT are actually not used for interrupts per se, but they're actually used for CPU fault handlers. And then the interrupt vector table part of the data structure begins after the CPU fault handler table.

When we actually go to handle interrupts, the handling occurs in the kernel. And this is done with 2 levels of interrupt handling: the Fast Interrupt Handler and the Slow Interrupt Handler. The Fast Interrupt Handler is the piece of code that's invoked directly from the interrupt vector table whenever an interrupt occurs. This is the piece of code that the CPU is just going to jump to when an interrupt occurs. Fast handlers execute in real time, and they're called fast interrupt handlers because they need to be fast.

The execution of one of these interrupt handlers needs to be short. If any large scale data transfer needs to occur, say we need to get a lot of data from the device all at once, this operation is handled by having the fast interrupt handler enqueue something called a task into the operating system's task queue. Whenever all the fast interrupt handlers for all the different interrupts are done executing, the CPU will go and check the task queue and execute any tasks that are present there. The part of interrupt handling that goes into the task queue is called the slow interrupt handler, and it's called this because it's not executed immediately and it can be interrupted by other devices. So what happens if an interrupt request is received while we're still processing an interrupt from a previous request?

Well, there's no problem if we're in the slow interrupt handler because this processing is done in such a way that we can stop this processing and handle the new interrupt if necessary. What happens if we are still running a fast interrupt handler? Well, the new interrupt handler could be executed before the first interrupt handler is finished, and this could cause some major problems, especially if we get a new interrupt from a device that's sharing the same interrupt line as the one we're handling. So what we do is we make fast interrupt handling atomic. That is, we make it uninterruptible.

On a single core system, this is as simple as disabling interrupts as long as we're running a fast interrupt handler. On multicore systems, there are actual special machine instructions to facilitate atomic operations that are pegged to 1 CPU core. These atomic interrupt handling operations will run to completion without interruption by any other interrupt or any other request on the system. Thus, the longer an interrupt handler takes to run, the longer the system will be unresponsive to any new interrupts. So what happens if a fast interrupt handler is coded in such a way that it takes too long?

Other devices might be requesting attention at the same time that this long running interrupt handler is executing. Worse, the same device that generated the original interrupt might now have more data to deliver to the OS before all the previous data is completely received. This could cause hardware failures, buffer overflows, dropped data, dropped messages, all kinds of issues at the hardware level. However, inside the operating system, this could lead to something called an interrupt storm, which is really bad. An interrupt storm occurs whenever another interrupt is always waiting to be processed whenever a Fast Interrupt handler finishes its execution.

That could occur either because the Fast Interrupt handler is too long and needs to be split into a Fast and a Slow handler. This can also occur if hardware has certain bugs that cause it to erase spurious interrupts. If the operating system is perpetually handling interrupts, it never runs any application code. Thus, it never appears to respond to any user inputs. The result of this situation is something called a live lock.

The system is still running, it's still processing all these interrupts. However, it's not doing any useful work. Thus to the user, the system appears to be frozen. And when an interrupt storm occurs and this LiveLock situation results, the typical way out of this problem involves judicious use of the power button. So interrupt handling is an important concept in order to support multiprogramming systems.

And interrupt handling, when these interrupt messages come through from hardware, is divided into 2 types of handler so that we don't get the interrupt storm. The fast interrupt handler executes atomically without being interrupted by anything else, but it must be fast, must enter that interrupt handler, do some very short operations, and immediately exit that handler. If any long running operations need to occur as a result of a device interrupt request, we need to handle those operations inside the slow interrupt handler. In this lecture, I will discuss finite state machines and UML state diagrams. UML state diagrams provide a way to model behavior of a system by analyzing how the state of the system changes in response to input data.

In this diagram, we have a model of a simple system that captures the user entering a number consisting only of the digits 0 through 9, and at most one decimal point. The system finishes and presumably returns the entered number whenever the user presses the enter key. Invalid keystrokes are simply ignored. With a simple state state machine such as the one presented here, we can verify that the user has entered only numeric data. However, understanding how the state machine operates and how UML provides us with a convenient way to draw the state machine requires some explanation.

First, we begin by observing that every software system has something that we call state. In simple terms, software state consists of the current values of all the variables in a program at any given moment in time. In the case of object oriented software systems, the state of the system is stored within the objects in the system, and each object has its own state, which is stored in the objects variables or fields. The essence of running a computer program is changing the software from one state to another. A computer, at least as we know it today, is a machine capable of representing an extremely large but finite number of different states.

Computer programs simply move the computer system from one state to another state. The conceptual way to model a simple computer or a single object in a software system is to use a finite state machine. Finite state machines have a fixed number of states, and the system is always in one of these states when it is running. As computation progresses, the finite state machine transitions from one state to the next. Each state machine can receive input, and a state machine possibly produces output during a state transition, upon entering a state, upon leaving a state, or even within a state.

For simplicity, the conceptual model used for input and output data is a tape. The state machine can read characters from or write characters to, one of these tapes. The set of characters that a machine knows how to read or write is called an alphabet. Words are formed from this alphabet, creating a language that the state machine recognizes. Nearly all state machines have an input tape.

Technically speaking, a finite state machine with only an input tape and no output tape is called a finite state automaton. This type of machine simply accepts or rejects the input, validating whether or not it is a recognized word in the language. A finite state transducer has 2 tapes, one for reading input and one for writing output. State machines can be either deterministic or non deterministic. A deterministic finite state machine transitions from its present state to another state based solely upon present state and the most recent input character read from the input tape.

In contrast, a non deterministic finite state machine can transition from one state to another on its own, without consuming any input data. These types of transitions are called Lambda transitions, or epsilon transitions in programming language theory. A state machine that produces output can be limited to producing output only upon entering a new state. This type of a state machine is known as a war machine and is theoretically easier to model. If a state machine can produce output at any time within a state or during a state transition, it is called a Mealy machine.

By now, you might be thinking that state machines sound rather limited in their capabilities, and this is a correct observation for the simple finite state machine. Both deterministic and nondeterministic finite state machines are equally powerful, but their power is limited to recognizing inputs that could be described using a regular expression. To obtain the power to process computer programming languages, which typically utilize what are known as context free grammars, we must add a stack to our finite state machine to create a pushdown automaton. Alternatively, if we modify a finite state machine to be able to move back and forth along a tape used for both input and output, we create a linearly bounded automaton that can handle context sensitive languages as input. The linearly bounded automaton is the closest theoretical state machine to the present day computer.

The final type of state machine is the Turing machine, which further modifies the linearly bounded automaton to make the tape infinitely long, giving it infinite memory. Such a machine, if it existed, could handle recursively innumerable languages. The unified modeling language allows us to diagram deterministic finite state machines, which allows us to model many of the operations individual objects in a large scale system perform. In the practice of writing software, we try to minimize non determinism since non deterministic programs could produce different results each time they are run with the same inputs. Therefore, we would like our enterprise applications to be deterministic.

UML state diagrams are used to model the behavior of individual objects within the system under discussion. One diagram is generally used for each object being modeled, so a normal set of design documents would contain multiple state diagrams. State machines expressed by UML either can produce output at any time, or the designers can restrict the output to the entry condition for each state. Thus, these state machines can use either the Mealy or Moore model. I'm now going to illustrate the components of a UML state diagram.

First, we have the initial state and the final state of the system. These states represent the entry and exit points for the diagram, respectively. The initial state, or diagram entry point, is represented by a filled circle. The final state, or diagram exit point, is represented by a filled circle within another circle. Internal states within the UML state diagram are given by a rounded rectangle divided into 2 sections.

In the top section, a human readable name for the state is generally provided. This name should reflect the purpose of the state. Below the name, the bottom section of the state rectangle contains the set of activities that the system performs while in the listed state. These activities are specified by providing a condition under which the activity is performed, followed by a slash, followed by the name of the action that is to be executed. Three special condition names are reserved by the UML specification to handle specific activities.

These are entry, which specifies an action to take upon entering the state, exit, which specifies an action to take upon exiting the state and do, which specifies an action to take while in the state. Note that a do action may be a continuous ongoing action that occurs until the system leaves the state. Transitions between states are drawn as arrows originating from one state and pointing to the next state. The system follows the arrows from state to state until it reaches the final state. A transition is taken only if the input to the machine matches a trigger condition specified next to the transition arrow.

This trigger condition can be accompanied by an optional guard expression enclosed in square brackets, in which case the transition will be taken only if both the trigger matches and the guard expression is true. After the trigger, an optional guard expression is placed a slash, followed by any action to take during the transition. The slash is always present in the UML state diagram, even if there is no action given. UML does allow a state with any UML state diagram to contain another state diagram instead of a list of activities. These so called nested state diagrams are permitted, but they will quickly make a UML state diagram difficult to draw and read.

Instead of nesting state diagrams, I suggest using a special symbol consisting of 2 empty states joined together as a way of indicating the state within the state diagram is itself modeled by another state diagram. Then you can draw a second state diagram for the complex state. I will now summarize what we have learned by discussing the meaning of this simple UML state diagram. First, the state machine starts in the initial state. It transitions to the no decimal state only upon receiving a digit in the range 0 through 9 as input.

Upon arriving in the no decimal state, the machine compends the received input digit to the output. The machine stays in this state until one of 3 types of input is observed. Another digit, a period for a decimal point, or the enter key pressed. If another digit is received, the machine exits, then reenters the no decimal state. Upon reentry, the new digit is appended to the output.

If the period key is pressed, the machine exits the no decimal state and enters the have decimal state, appending the decimal point to the output during the transition. From the have decimal state, the system will exit and re enter the have decimal state whenever the digits 0 through 9 are received appending the received digit to the output upon the transition. From either state, pressing the enter key ends the machine by moving to the final state. Also, any invalid data to the machine is ignored at all states, since no trigger is matched. In short, this finite state machine implements logic that reads in and stores either an integer or a simple floating point number In this lecture, I will introduce dynamic memory allocation at the system level.

This method of memory allocation improves the degree of multi programming that a system can provide by allocating memory to processes as needed, instead of ahead of time in fixed size chunks. Fixed memory partitioning is fast and efficient in terms of overhead, but it wastes space and limits the number of concurrent processes to the number of partitions that will fit into RAM. Dynamic memory allocation resolves this issue by allocating memory to processes as it is needed. This mechanism increases the degree of multi programming that the kernel could support, but this improvement comes at a cost of greater complexity. In addition, there are trade offs present in dynamic memory allocation that directly affect the performance of the system.

These issues include efficiency, methods for tracking free space, algorithms for determining where to make the next allocation, and memory fragmentation. The primary issue with dynamic memory allocation is fragmentation. During execution, processes allocate and free memory in chunks of varying sizes. Over time, regions of free space within memory become non contiguous, the sections of allocated memory in between sections of available memory. This fragmentation could lead to serious performance issues since program execution speed will drop dramatically if every data structure must be implemented as a linked list of small pieces.

Furthermore, algorithms that try to perform on the fly memory defragmentation are extremely complex to implement and would also severely impact system performance. Since it is impractical to avoid or fix fragmentation, memory fragments are a significant concern when space is dynamically allocated. Small fragments of memory are useless to programs, particularly c and c plus plus programs, which require structures and objects to be allocated in contiguous memory regions. These structures are likely to be in various sizes that are not efficient to utilize for memory management purposes, so the systems will allocate a few more bytes than the structures require in order to improve efficiency of the memory tracking systems. Furthermore, structures and objects will be allocated and freed numerous times during program execution, further fragmenting the RAM.

Over time, fragmentation can waste large portions of system memory, limiting the degree of multi programming by making it impossible for new processes to start. There are 2 types of fragmentation: external and internal. External fragmentation occurs when free space in memory is broken into small pieces. Over time, as blocks of memory are allocated and deallocated, this type of fragmentation tends to become worse. External fragmentation is so bad with some allocation algorithms that for every n blocks of memory that are allocated to a process, the system wastes another half n blocks and fragments.

With these algorithms, up to a third of system memory becomes unusable. The other type of fragmentation is called internal fragmentation. Internal fragmentation occurs because memory is normally allocated to processes in some fixed block size. The block size is normally a power of 2 in order to make kernel memory tracking structures more efficient. Processes tend to request pieces of memory in sizes that do not fit neatly into these blocks.

Thus, it is often the case that parts of a block are wasted as a result. For small memory requests, large portions of the block may be wasted. When a process requests heap memory, the kernel must find a chunk of free space large enough to accommodate the request. This chunk cannot be smaller than the requested size since the process expects to use all the space it is requesting. Since the memory is divided into blocks for efficiency, the chunk of memory returned to the process is normally larger than the amount requested, unless the process happens to request a whole number multiple of the block size.

I'm now going to introduce 4 classical algorithms for dynamic memory allocation, best fit, worst fit, first fit, and next fit. The first classic algorithm for dynamic memory allocation is the best fit algorithm. In this algorithm, the kernel searches for the smallest chunk of free space that is big enough to accommodate the memory requests. Although best fit minimizes internal fragmentation by avoiding over application, External fragmentation is a major problem. An attempt to reduce the external fragmentation in the best fit algorithm is observed in the somewhat counterintuitive worst fit algorithm.

Using the worst fit algorithm, the kernel finds and allocates the largest available chunk of free space, provided it is large enough to accommodate the request. In theory, this allocation strategy leaves larger, and thus potentially more usable, chunks of free space available. In practice, however, this algorithm still fragments badly, both internally and externally. Aside from the fragmentation, both of these algorithms are impractical in actual kernel implementations because they must perform a search of the entire free list to find the smallest or largest chunk of memory. The need to search the free list is eliminated using the 1st fit or next fit algorithm.

In the 1st fit algorithm, the kernel simply finds and allocates the first chunk of memory that is large enough to satisfy the requests. This approach does result in internal fragmentation, and it also tends to create small fragments of free space that accumulate at the start of the free list, reducing performance over time. The next fit algorithm avoids the external fragment accumulation by starting the search for the next chunk of memory from the most recent allocation. In practice, only the first fit algorithm is used in the Linux kernel and then only for embedded devices. This algorithm is called the SLOB allocator, which stands for simple list of blocks.

For non embedded systems that have the computational power for a more complex algorithm, slab allocation is used instead of any of these simple algorithms. In this lecture, I continue the discussion of dynamic memory allocation. I will introduce the power of 2 methods with the buddy system and coalescence for allocating memory to processes. Then I will introduce slab allocation, which is used within the kernel to allocate kernel data structures efficiently. In the previous lecture, I introduced the classic algorithms for memory allocation: best fit, worst fit, first fit, and next fit.

Now I want to introduce algorithms that are actually used within OS kernels to perform memory allocations efficiently. These algorithms are called power of 2 methods, and they work by maintaining information about allocated and free blocks in a binary tree instead of a list. At the top level, memory is divided into large blocks called super blocks. As processes request memory, these super blocks are divided into smaller sub blocks from which the memory is allocated. Sub blocks can be further divided creating a hierarchy of block sizes.

Algorithms based on this method are relatively fast and scale to multiple parallel CPU cores. These algorithms also reduce external fragmentation by coalescing free blocks as I will discuss in a few moments. Some internal fragmentation does still occur, however. In this diagram, we can see 3 superblocks, 2 of which are partially in use. Each of the first two superblocks has been divided into 3 subblocks, and the 3rd subblock of the first superblock has been further divided.

When a process requests memory, the kernel must perform a search of the tree to find an appropriately sized block to handle the request. In performing the search, the kernel might subdivide an existing block into a smaller block to reduce the amount of internal fragmentation. Since this is a search process, a reasonably powerful CPU is required to make this operation efficient. Another improvement to memory management is to utilize a buddy system and the power of 2 strategy. In this allocation system, 2 limits are chosen to be powers of 2: the upper limit u, and the lower limit l.

The super blocks are the blocks of size u, and these blocks can be subdivided into blocks as small as l bytes. There are trade offs in picking the size to use for l. A smaller size produces less internal fragmentation since the block size more closely matches the smallest request sizes from the processes. However, a smaller size for l means there are more total blocks to be tracked, which increases the size of the binary tree, using war RAM to store the tree and increasing the search time. On the other hand, a larger size for L reduces the search time and makes the tree smaller, but the amount of internal fragmentation increases.

In addition to the block size limits, the buddy system also uses a technique called coalescence. Whenever a process frees a block, the kernel checks to see if either neighboring blocks is free also. If 1 or more neighbors or buddy blocks are free, the block is coalesced into a larger block, reducing external fragmentation. The coalescence algorithm is efficient since the maximum number of coalescence operations that must be performed is equal to the base two logarithm of U divided by L. By properties of logarithms, this value is equivalent to the base 2 log of U minus the base 2 log of L.

Thus, for example, a system with a maximum block size u of 4,096 bytes, or 212, and a minimum block size of 512 bytes, or 29, will require at most 3 coalescence operations to recreate the superblock. Returning a single 512 byte block whose neighboring 512 byte buddy is free will cause the 2 blocks to be coalesced into a single 1024 byte block. If the neighboring 1024 byte block is free, the 20 24 byte blocks will be coalesced into a 2 1,048 byte block. Then, if a buddy 2,048 byte block is free, the 3rd coalescence will produce a 4,096 byte superblock. The power of 2 methods are useful for allocating memory to processes where some internal fragmentation is acceptable.

However, within the kernel, it is preferable to minimize both internal and external fragmentation to avoid wasting space. This conservative approach is needed since the kernel is always mapped into main memory. An efficient solution for allocating kernel memory is to use a slab allocation algorithm, in which kernel memory is arranged into fixed sized slabs. Each slab is divided into regions sized for specific types of kernel objects, including file descriptors, semaphores, process control structures, and other internal data structures. Initial layout of these slabs is performed at compile time.

At run time, several of each of the different slab layouts are pre allocated into caches. Whenever the kernel requires a new data structure, space for the data structure is simply taken from the slab cache. If the slab cache starts to run out of certain slab layout, it automatically provisions extras. Graphically, slabs can be represented in a manner shown here. In this example, we have 2 pre allocated copies of the same slab layout in which each slab can hold a single instance of each of 5 different kernel objects.

Some wasted memory does occur with this arrangement since there might be a larger number of one type of object than of another type of object. However, this approach is generally more efficient in terms of kernel space utilization. Slab allocation does require more CPU power than does a classical method such as first fit. Thus, in some embedded environments, the slab allocator might be preferable. If slab allocation is chosen over the slab allocator, Linux kernel has two choices of slab allocator.

The first choice is the original slab allocator, which was the default allocator until kernel version 2.6.23. This allocator performed well on shared memory systems with few CPU cores, but wasted considerable memory space when used on extremely large shared memory systems, such as those found in graphics rendering farms. To reduce the space waste on large scale SMA systems, Christoph Lameter at Silicon Graphics developed a new allocator called SLUB, which reduces the size of data structures needed to track allocated and free objects. The initial implementation of the SLOB allocator contained a performance bug that affected the results of certain memory benchmarking tools. Initially, Christophe believed the bug was of little importance since the conditions required to trigger it were fairly uncommon in practice.

However, Linus informed Christophe that either the problem would be fixed or SLUB would be dropped entirely from the kernel. In the end, it was determined that the bug was caused by adding partially used slabs at the beginning of a linked list instead of to the end of that list. The fix was a change to one line of code, and the slab allocator has been the default Linux allocator since since 2.6.23. In this lecture, I will introduce memory resources and how the system and its processes view random access memory. In order to run any process or instance of a program on a computer system, we need to provide 2 critical resources: access to the CPU and access to random access memory or RAM for storing and manipulating data.

RAM is a type of dedicated hardware memory that is attached to the motherboard. It is separate from persistent storage or our disk space. RAM is also volatile, which means that it loses its contents whenever power is interrupted to the RAM modules, including whenever the computer is turned off. At a low level, the hardware presents memory to the operating system as one large block of space. This space is divided into bytes, and each byte in memory has a unique address that can be used to access it.

A 32 bit architecture provides enough of these byte addresses to utilize up to 4 gigabytes of RAM. Above that amount there is not enough space in a 32 bit number to store the address of any memory locations in excess of 4 gigabytes. Thus, a 64 bit architecture is required for systems with more than 4 gigabytes of RAM. Each process or running instance of a program on a system uses a different view of memory. Process memory is divided into several pieces: the stack, the heap, global variables, and the text segment.

The stack is used to store automatic variables, or variables that are local functions in the C programming language. Space on the heap is manually allocated and deallocated by the programmer when writing C code using the functions malloc and free. Free space for extra data is located between the stack and the heap, and the stack and the heap grow toward one another. Global variables are provided with their own section of memory, which is allocated between the heap and text segment. The text segment is used to store program code.

This segment of memory is read only and cannot be changed. As I mentioned in the previous slide, automatic variables are placed onto the stack. This placement is performed by the compiler when the program is built. Placement of data onto the heap is historically performed by the programmer, although many of these operations are now automated in modern dynamic languages. In the C and C plus plus languages, the programmer must explicitly request and release or allocate and deallocate heap space.

In C, these operations are performed using the malloc and free functions, while the new and delete operators are used in C plus plus In Java, the programmer must explicitly allocate space on the heap using the new keyword. However, the Java runtime automatically determines which heap allocations are no longer in use and frees those locations automatically. This process is called garbage collection. Python provides both automatic allocation and garbage collection. Whenever a data structure requires heap space, the Python interpreter allocates it automatically.

Once the data structure is no longer in use by the program, the garbage collector deallocates it without programmer intervention. Use of heap memory and processes presents a challenge to the operating system because these allocations and deallocations are not known in advance. The compiler is able to track and report the amount of stack space needed, since the number of local variables in a function never changes in a language like C. However, the number of heap allocations may vary from program execution to program execution, making it impossible to know exactly how much space should be allocated in advance. Worse, heap memory allocations tend to be small and frequent, so the process of allocating memory from this section needs to be fast.

This speed and dynamic availability needs to be maintained while the operating system shares the computer's RAM among multiple processes at the same time. In addition to sharing memory among processes, the kernel has memory requirements of its own. Aside from the kernel code and its own automatic variables, the kernel is filled with data structures that dynamically grow and shrink during the course of system operation. These data structures are numerous and include process control blocks, the ready list, scheduling views, device access tables, and many other types of structure. Unlike some processes, however, all memory allocation and deallocation in the kernel is performed by the kernel programmers.

In the Linux kernel, programmers can use one of a number of functions, including k malloc, kzalloc, vmalloc, kfree, and vfree. The first issue that must be solved by the kernel is sharing memory between itself and a user space process. This sharing is accomplished first by dividing the memory into 2 regions: kernel memory and user memory. The kernel keeps its data structures, program code, global variables, and automatic variables in kernel memory, isolating these items from the running process. Process memory is placed in a separate region from the kernel, but the kernel is always available in memory.

This mapping is necessary to maintain performance whenever an interrupt occurs, the process makes a system call to the kernel, or the process experiences a fault that must be handled by the kernel. So how do we go about running multiple processes at the same time? Well, the first way we could consider, which is used in some types of embedded systems, is to divide the memory into fixed size chunks called partitions. A memory partition is a single region of RAM that is provided to a single process. Both the program code and data associated with each process must fit within this pre allocated space.

If a process grows too large, it will run out of memory and crash. Furthermore, the maximum number of concurrent processes, called the degree of multi programming, is limited by the number of partitions available. Once all memory partitions are used, the system cannot start any new processes. This situation is exacerbated by the fact that small processes may not be using their entire partitions, resulting in wasted memory. In this lecture, I will introduce paging and related topics including logical addressing, address translation, and the translation look aside buffer.

Paging provides a mechanism for sharing memory among multiple user space processes at the same time. This mechanism improves upon simpler algorithms such as static partitioning and direct power of 2 methods by allocating fixed size pages of memory to processes. The key to effective memory utilization with paging is that each process is given its own logical memory space. In other words, each process has its own view of memory with its own address space. The addresses that the process sees are called logical addresses.

These logical addresses are divided into fixed size pages. Each process in the system receives its own private set of pages with private memory addresses. When a process accesses memory using one of its logical addresses, the CPU translates the logical address into a physical address. Physical addresses refer to locations in system memory. For performance reasons, translation is done in terms of memory frames, or fixed size regions of RAM.

The base frame size is normally 4 kibibytes, although this can vary by hardware device, and most hardware can support multiple different frame sizes. Operating systems normally use logical page sizes that correspond to the Linux kernel can support so called huge pages, which can be as large as 1 gigabyte when using the newest AMD and Intel CPUs. The key advantage to paging is that it eliminates the issue of external fragmentation. Since the CPU is translating logical page based addresses into physical frame based addresses anyway, there is no need for the physical frames to be contiguous. As a result, we can store a data structure and process memory using pages that are logically contiguous.

However, when these logical pages are mapped to physical frames, the frames may be scattered throughout RAM. Notice that the distinction between a page and a frame is a matter of terminology. A page refers to a block of logical memory, while a frame refers to a block of physical memory. For now, pretend that there is a one to one correspondence between logical pages and physical frames. We'll make things more complicated later.

The key to making page translation efficient is that the CPU contains special hardware called the memory management unit or MMU, which performs the translation operations. In this diagram, the process accesses memory using logical addresses, which are divided into pages. When requests are made using these addresses, the memory management unit on the CPU translates the logical address into a corresponding physical address. The resulting physical address will be to some point of a physical memory frame. Note that individual memory addresses within pages or frames still remain contiguous, which is important because the MMU translates page numbers to frame numbers, leaving the offset to the memory location within the page unchanged.

As shown in this diagram, we can divide a logical address from a process into 2 components: the page number p and the offset d. When the MMU is asked to perform a translation it consults a data structure called a page table, which provides a mapping between page number and frame numbers. Using this information, the MMU constructs the physical address by using the corresponding frame number, represented here by the letter f, in place of the page number. Once again, the offset to the particular byte within the page or frame is left unchanged. A particular byte in RAM is actually addressed using the frame number and offset into the frame.

However, to the process, this memory access appears to occur using a logical memory address, which is conceptually divided into a page number and offset. The offset component is not changed by the MMU, but the page number is replaced by the physical frame number. In order to perform the translation from page numbers to frame numbers, the MMU must consult the page table. The page table is a data structure that is itself stored in RAM in the kernel memory space. Storing the page table in RAM leads to a major problem, since every MMU translation would require a lookup.

Since the lookup requires a memory access, each process memory request would actually require 2 physical memory accesses. This situation is especially troublesome because a memory access occurs both on accessing data and upon reading the next instruction to be executed. Without some additional hardware to resolve this problem, system memory performance would be effectively cut in 2, greatly reducing the overall performance of the system. The solution for eliminating the double memory access issue is to add a component to the CPU called the translation look aside buffer or TLB, which stores some page frame mappings. Some TLBs also provide room for address space identifiers which aid in implementing memory protection.

The TLB is a piece of associative memory, meaning that it can perform rapid parallel searches, resulting in constant time lookups for page translation. This memory is exceptionally fast, meaning that it is also quite expensive. As a result, TLB sizes are typically limited from 8 to 4,096 entries. The addition of the TLB provides a potential shortcut for performing address translation. Instead of immediately searching the page table, the MMU first searches the TLB.

If the page to frame mapping can be found in the TLB, then it is used to perform the translation from page number p to frame number f. This situation is called a TLB hit. A TLB miss occurs whenever the page number is not present in the TLB. In this case, the MMU must search the page table to locate the appropriate frame number. The CPU and operating system employ various policies to determine when to store page to frame mapping in the TLB.

A simple policy would be to use a 1st in first out policy that replaces the earliest entry in the TLB with the newest entry upon a TLB miss. Other, more complex, and potentially better policies also exist. In this lecture, I will discuss memory protection, including segmentation and permission bits on page table entries. Remember that operating systems perform 2 functions: abstraction and arbitration. Mechanisms for accessing memory provide abstractions of the underlying memory hardware.

However, operating systems must also arbitrate access to RAM by ensuring that one process cannot access memory that does not belong to it. Without this arbitration, a process could change memory belonging to another process or worse it could crash the system by changing memory that belongs to the kernel. On systems that utilize simple memory management such as power of 2 methods, memory access protections are provided by a mechanism called segmentation. On the majority of modern systems which employ paging, memory protection is implemented as part of the paging system, and memory access permissions are stored in the page table. On any system, process memory is divided into logical pieces or segments at compile time.

These segments include the text segment, a region for global variables, a stacked region for automatic variables, and a heap for dynamically allocated data structures. Access permissions apply to each segment. In particular, text segment is set to be read only. Outside a single process, there must be a mechanism to track which segments of memory belong to which processes. When a process is executing, its segments are marked valid so that it can access the corresponding memory locations.

Segments of memory belonging to other processes are marked invalid, and any attempt to access those segments results in a fault or interrupt called a segmentation fault. Typically a segmentation fault causes the process to be terminated. Segment memory permissions are implemented on non paging systems using a segment table. The segment table has permission bits that can be applied to each region of memory. When memory is accessed using segmentation, the segment table must be consulted to determine whether or not the access is legal.

In this example, a process requests access to memory using a logical address. Since we do not have paging with the system, this logical address is not translated by a page table mechanism. However, this logical address is divided into a segment address and an offset into the segment in a manner similar to page translation. The MMU checks the segment table to determine if a particular memory access is valid. In this example, the segment table stores up to 4 permissions and up to 3 bits: a validinvalid bit, a read write bit, and an execute bit.

In practice, most systems that support segmentation without gauging normally only use 2 bits: valid, invalid, and read write. If the process tries to read from a segment that is marked valid, the memory access is permitted and occurs normal. The same thing happens if a process tries to write to a memory location that is marked both valid and writable. However, if a process tries to write to a segment marked read only or if a process tries to access an invalid segment, the CPU triggers a segmentation fault and the process is terminated. For some invalid accesses on a UNIX like system, this segmentation fault may be reported as a bus error.

With paging systems, which comprise the majority of modern systems, including mobile devices, memory protection is accomplished by adding permission bits to the page table entries. In general, page table entries will have a valid invalid bit and a read write bit. The valid invalid bit is used in the same way as it is for segmentation. Pages that a process is allowed to access are marked valid. Other pages and any non existent pages are marked invalid.

If a process attempts to access an invalid page, a CPU fault is raised, which functions like an interrupt to trap into the kernel. A Linux kernel will send a SIGSEGV or SIGBUS signal to the process depending on the location in memory the process tried to access. In practice, the signal is normally not caught and the process terminates. For historical reasons, this event is called the segmentation fault or seg fault for short. The read write bit used to mark the text segment of a process can be used to allow pages of memory to be shared between processes.

Pages that are reentrant or read only can be accessed by multiple instances of multiple programs simultaneously. This capability is useful on modern systems since multiple instances of programs are typically run at the same time. In the case of a web browser, for example, it is only necessary to load one copy of the browser program code into memory. Several copies of the browser can be run as several different processes sharing the program code and thus saving memory. The open source Chromium browser and its Google Chrome derivative allow each tab to run-in a separate process.

Shared memory pages allow the code for the browser, any extensions, and any plugins to be loaded only once, saving memory. This diagram illustrates how 2 processes can share a single page in RAM. Each process sees a handful of valid frames, one of which is marked read only. If this memory frame contains code or other information that can be shared between the processes, then the two frame numbers will be identical within the separate processes. Each process may use a different page number to represent this memory location, however, since each process has its own independent logical view of memory.

Incidentally, this diagram is a conceptual diagram only. It does not directly map to any particular data structure in the operating system. Instead, the two tables illustrate how each process might see page table. Newer AMD and Intel CPUs support an additional permission bit for setting execute permissions. This bit called the no execute or NX bit is actually an inverted permission.

It is set to 1 whenever execution of data found on a memory page is forbidden. Originally, the NX bit was implemented by AMD on its 64 bit capable processors using the marketing name of Enhanced Virus Protection. Intel followed suit and added this mechanism as the execute disable or Xd bit. The concept behind the bit was to provide a mechanism that could be used to prevent execution of native machine instructions from memory space used for regular data. Although the primary beneficiary of this feature was a certain virus prone system that is not a UNIX variant, the Linux kernel does support the NX bit as a guard against buffer overflow and similar exploits.

In the example presented in the hypothetical page table here, only the page with hex numbers 04a4 allows code execution. In the event of an exploit attempt, a malicious application could try to load code in another page, perhaps 04A1. However, since the NX bit is set on that page, any attempt to execute the code loaded by the exploit will trigger CPU fault and the process will be terminated. This mechanism increases the security of the system against certain types of attacks. In this lecture I will introduce test driven design.

I will begin by discussing the importance of testing, then I will introduce unit testing, followed by an overview of the test driven process. Testing computer software serves two purposes. 1st, it provides a way to detect and isolate bugs in the application. Any application less trivial than Hello World generally contains multiple bugs. As software matures through its life cycle, we expect the number of bugs in an application to decrease as issues are found and fixed.

Although the number of bugs in an application may decline over time, it is not good practice to release an initial version of a program with obvious and or major defects. Thus, testing should be a major component of the initial software design process, as well as an ongoing activity throughout the software lifecycle. Although we often associate testing with finding and eliminating bugs in the code, testing serves a second purpose that is at least as important. That is, testing verifies that an application actually satisfies its requirements and carries out the operations it is designed to perform. Testing demonstrates that our software is ultimately correct.

In fact, one principle of software development is that a working program, as evidenced by successful testing, is a proof of a solution to some kind of problem. Just as mathematicians use proofs to verify that steps in solving a problem are correct, we can treat each program as a proof that the steps in an algorithm are correct for solving a problem. I should emphasize here that despite the recent marketing of mathematical methods for software analysis in a number of professional publications in our field, testing remains the one and only way to verify that a program is ultimately correct. While mathematical methods such as formal verification and design by contract do have useful properties, such as ensuring that our specifications are not internally inconsistent, they do not replace testing. In fact, showing that a mathematical model correctly represents a solution to a problem is mathematically equivalent to writing a correct program.

Furthermore, we have already stated that a correct program is a proof. So how can these tools truly revolutionize the way we program to the point where programming as we know it becomes extinct? In other words, can we use the formal methods effectively and efficiently to replace good programming techniques? Well, the first problem is that it takes roughly 3 times as many symbols to model a program as it does to implement the program. As Peregrine Hanson once noted, how can you expect to get formal approaches working correctly with 3 n symbols, if you can't write a correct program with n symbols.

The larger issue, however, is that all the formal approaches are effectively applying various algorithms to the process. Perhaps these algorithms could save us some time by figuring out whether or not our program design will be correct in advance. Or to put it mathematically, perhaps these methods could determine in advance if our proof will work before we actually do the proof by writing the program. Turns out that David Hilbert had the same question about mathematical proofs back in 1928. The question, which he called the Entscheidens problem, asked if it is possible for an algorithm to determine whether or not a proof would be successful before actually doing the proof.

Within a decade, Alan Turing proved the halting problem was reducible to the inscriptions problem, and he also proved the halting problem is not solvable algorithmically. Thus, the answer is no, or to put it succinctly, there is no substitute for testing. There are a number of different ways that we can test a program, and these methods correspond to different types of testing. For test driven design, we are primarily concerned with a type of testing called unit testing. Unit testing involves testing each component of the application in isolation, ensuring that every component works by itself before trying to integrate it with other components.

In an object oriented programming paradigm, a component the program is typically a single class. We write unit tests as assertions, or conditions that will be either true or false. We say that a unit test passes whenever the result of a test matches its expected result. When a test result does not match its expected result, the test fails. In performing unit testing, we normally use a combination of positive and negative test cases.

Positive test cases expect the result to have a value of true, while negative test cases expect a false answer. We normally automate unit testing by using some kind of unit testing tool that runs the test cases for us and produces a report. A large number of automated unit testing tools exist. One popular class of unit testing tools is called XUnit. These are technically completely different tools implemented for testing components written in completely different languages.

These tools tend to operate in a similar way. Among the different xUnit tools, we have jUnit for Java applications, pyUnit for Python applications, PHPUnit for PHP code, nUnit for C applications, and cppUnit for C plus plus code. The process of using a test driven design with unit testing is illustrated in this UML activity diagram. This diagram shows a single iteration of a test driven process. Test driven techniques are examples of iterative and incremental development models.

We begin by designing a test case which must initially fail as I will discuss in a moment. If the test case does not initially fail, we continue the design step until we develop a test case that does fail. Once the test case fails, we write code for the application until the test case passes when we run the application against the test case. We run the test as many times as necessary until the test passes. After the test passes, we refactor the code to make it readable clean.

We then run all the test cases we've ever written for the system and verify that all the test cases pass. If not all the test cases pass, we still have work to do on our new code. Once all the test cases do pass, our iteration is complete. As I mentioned a moment ago, the first step in an iteration using a test driven process is to create a test case that fails. At first, this may seem counterintuitive.

Why do we want a test case that fails? Well remember that we haven't yet written any code from our test driven design. If we design a test case that immediately passes, we've made 1 of 2 mistakes. Either the test case is wrong, or the program already does whatever the test is checking. If the test is wrong, we need to fix it so that we don't code to an incorrect test.

On the other hand, if the test is correct and the software already passes the test case, then we have no new code to write since the feature is already present in the application. Once we have a test case that fails, we write code that extends the application to pass the new test case. This code should be as simple as possible and it should be written to the test. It should make the application pass the test, nothing more. Also, this initial code does not need to be pretty, have correct styling, except in Python, or be especially readable at first.

The goal here is to pass the test. Once the test passes, we refactor the code to meet style guidelines and make it readable. Refactoring means that we change the code, and perhaps the design, to make the implementation cleaner without changing the functional behavior of the program. We need to ensure that our refactoring process has not broken the code by ensuring that the test case still passes. Once we finish the refactoring process, we know that the application passes the new test case.

However, our new code may have introduced 1 or more regressions in the application, meaning that our new code may have broken something that used to work. To check for these regressions, we must rerun every single test case that we have ever written for the application. It is for this reason that it is highly useful useful to be able to automate the testing. If our testing finds no regressions, then our iteration is complete, and we can move on to the next iteration in the process by designing a new failing test case for a new feature to be implemented. However, if regressions are present, we must fix the regressions.

If the code cannot be fixed in a reasonable time period, we revert the code to the previous version and try the process again. In other words, we throw out the code we just wrote and try again. While throwing out the code may seem like a waste, it is sometimes faster to try the implementation again instead of becoming bogged down in debugging. This economy of speed is especially valued when using agile methods such as extreme programming. One final thing to consider about test driven design is that, like any other development technique, it is not the perfect solution to every problem on the planet.

Always be wary of claims that any process or technique, including test driven design, can somehow revolutionize software development. Like every other technique, this approach has its limitations. One limitation is the test driven design depends upon unit testing, and unit testing has the same power as using assertions in code. Everything we test with the unit test needs to be reducible to a true false decision, which means that we cannot unit test some aspects of the system, such as the graphical user interface. Another limitation of test driven design is the same limitation that we have with software engineering in general.

If the specification or high level design is wrong, test driven design cannot fix it. If you implement a test case using an incorrect specification, that test case will itself be wrong. Writing code that passes an incorrect test case will implement a non feature, and the program will be wrong. Finally, the test cases must be maintained over the life of the software, since we need to be able to regression test after every change. If the test cases are not well written so that they are themselves maintainable, the test cases may become useless as the software evolves.

Should this happen, we will lose the ability to test for regressions, and future changes could easily break the application. In this lecture, I will discuss page tables. Page tables are data structures that store mappings between logical pages and process memory and physical frames in RAM. These structures are used and managed in different ways on different systems, often with assistance from the hardware. At the end of this lecture, I will discuss extended page tables which are useful for allowing hardware to support multiple simultaneous operating systems at once.

Recall from the previous lecture that the page table stores mappings between page numbers and frame numbers. Whenever a TLB mis occurs, the page table must be searched to find the appropriate mapping. CPUs have differing levels of support for managing and searching the page tables automatically. On most modern systems, including x8664 and ARM CPUs, page tables are managed and searched by the CPU automatically upon TLB miss. This search increases the memory access time, but no fault or interrupt is generated.

As a result, the CPU does not have to perform a context switch. Among a few other CPUs, the MIPS architecture requires the operating system to manage and search the page table. Whenever a TLB mis occurs, the CPU triggers a fault, which is a type of interrupt. The CPU must make a context switch away from whatever task is currently being executed in order to execute the interrupt handler for the fault. Software managed page tables are becoming increasingly uncommon even on embedded systems as the popular ARM CPU supports hardware management.

The MIP CPU is typically used in lower end consumer devices, such as inexpensive e readers and the least expensive tablets. One approach to reducing the page table search time whenever a TLB miss occurs is to store the page table as a tree instead of a list. This technique of hierarchical page tables divides the page tables into pages. Each logical address in process memory is divided into an outer page table, a set of offsets into various levels of sub tables, and a final offset to the specific byte of memory to be accessed. This technique can be generalized to any number of levels in the hierarchy, but I will present here a simple system that uses only 2 levels.

As illustrated in this diagram, the data structure is arranged so that an outer page table provides a mapping between outer page numbers and page table pages. Once the proper page table page is located, the translation from page number to frame number can be completed quickly, since the page of the inner page table is relatively small. The address translation mechanism used with hierarchical page tables is more complex than that used with a simple linear page table. The logical address is divided into additional components In this example, with two levels in the page table, the logical address is divided into an outer page table entry number, t, which specifies the location in the outer page table in which to find the proper inner page table. The next component of the address, p, is the offset into the inner page table at which the mapping can be found.

In this example, we have a single page to frame mapping in each inner page table entry, so we can obtain the frame number from that entry. A real system will be more complex and likely will require a short linear search at some level in a page table. Once the frame number is determined, RAM is accessed in exactly the same way as it is in simpler designs, with the final address joining frame number and offset into the physical address. Storing the page table in a tree improves access performance. However, as the total amount of system RAM continues to increase with newer and newer generations of computers, the size of the page table also increases.

Moreover, the address spaces on 64 bit architectures are much larger than the amount of memory that the MMU actually supports. Current 64 bit systems have true hardware address sizes in the range of 34 to 48 bits. If we were to store a mapping to handle every logical page number in such a system, the mapping would be large and inefficient since the address space is sparse. That is not every 64 bit logical address can map to a physical location in RAM since the physical addresses are at most 48 bits. As a result many of the possible 64 bit addresses are unused.

A solution to this problem, which both reduces page table storage size and increases search speed, is to use a hash table or dictionary structure to store the outer page table. Since several addresses may hash to the same value, each entry in the hash table is an inner linear page table, which allows the hash collisions to be resolved through chaining. Translating a logical address to a physical address with a hashed page table begins by hashing the page number p. Hashing is accomplished using a hash function, which may be implemented in hardware for high performance. The hash value returned by the function gives the location in the hash table where the inner page table may be found.

A linear search of the inner page table is performed to locate the frame number. Once the frame number f is obtained, it is joined with the offset d to give the hardware address. Some architectures, notably PowerPC and Intel Itanium, store their page tables backwards. That is, the system stores the data structure with 1 entry per frame, and the entry stores the corresponding page number along with the process ID of the process owning the page. This approach, called an inverted page table, is efficient in terms of page table storage size.

However, inverted page tables are inefficient in terms of performance, and these structures are not used on a majority of systems. On newer x8664 systems with virtualization extensions, hardware support exists for extended page tables or EPT. AMD and Intel each brand this technique with a different name. AMD uses the char rapid virtualization indexing or RVI on newer CPUs. They used to call this technique nested page tables or NPT.

Intel uses the extended page tables terminology. EPT adds a level of paging to the system. At the outer level each virtual machine or guest running on the CPU sees its own set of memory frames isolated from and independent of the actual hardware. The CPU translates page numbers to frame numbers first by translating the page number to a guest frame number. The guest frame number is then translated to a host frame number, which is the physical frame number.

EPT technology is important for virtual machine performance since it allows each guest to manage its own memory efficiently. Moreover, guests can access memory without having to switch the CPU into hypervisor mode, which can be an expensive operation. The downside to logical address translation with extended page tables is that it becomes conceptually more difficult to understand as illustrated by the complexity of this diagram. A process running in a guest operating system makes a memory request just as it would if no virtualization were present. This memory request is divided into a page number and an offset as usual.

The CPU then performs translation of this page number p to a frame number f also as usual. Furthermore, as far as the guest operating system is concerned, the translation is complete. The guest OS sees the memory region provided by the host system as if that memory were physical memory. In other words, the guest OS has no idea that it is running in a virtual machine. To the guest OS, the virtual machine looks just like a physical system.

However, in reality, the guest's physical memory is actually an illusion provided by the host. To make this illusion work, the host must translate frame number, f, in guest memory to an actual physical frame number, g. This translation is performed by the CPU without any context or mode switches using the EPT table. Once this translation is performed, the physical memory address is generated by combining G and D, where D is the original unchanged offset into the page. It is important to note that this entire process is performed by the CPU, without switching to the host OS or hypervisor.

In this lecture, I will introduce basic UML class diagrams. This introduction will be language independent and will focus on diagrams themselves. I will cover language specific uses of class diagrams in later lectures. UML class diagrams provide a graphical way to illustrate relationships between classes in an object oriented system. They also provide a way to draw an abstract diagram of a class so that we can inspect features visually without having to resort to looking at code.

Although class diagrams are a UML standard, in practice, there are differences between the diagrams produced by different tools. Not all tools support all UML features. Some tools add extra features, and the final appearance of diagrams drawn by different tools varies. Certain UML tools are capable of round trip engineering, which means they can draw UML class diagrams from existing code and generate program code from UML models. Generated code provides templates of classes that programmers can finish.

The ability to perform round trip engineering depends upon having a programming language the tool can parse or upon having a UML class diagram model in a format the tool can use. The basic entity in a UML class diagram is a class. Classes are represented in the simplest case by boxes In object oriented designs, the data members of a class are often called fields. Class fields are listed in a UML diagram between 2 horizontal lines under the class name inside the box representing the class. The first character in the field name gives the field scope, a plus sign for public scope and a minus sign for private scope.

There are other scopes which are represented by other symbols, but remember that different implementation languages support different scopes. Different programming languages also use different type systems. In the pure case, UML class diagrams can be drawn to be language independent. In practice, however, most UML class diagrams have language dependent semantics, which include the type system used in the target language. In this example, the Hello World class has one field a private member named message, which is of type Message with a capital m.

The capitalized Message refers to another class named Message, of which Message with a lowercase m is an instance. Methods or member functions in c plus plus speak go below the class fields in the bottom section of a class box. These methods are functions that are members of the class, which can be called to operate on instances of objects. Static methods, or methods that can stand alone without having to be called from an instance of a class, are underlined. The same scoping and typing conventions are used for methods as are used for fields.

As with fields, the exact semantics tend to vary from tool to tool, with each tool using a slightly different format. In this example, we have added 5 methods to our class, all of them public. We have setter and getter methods for the underlying message setMessage and getMessage, respectively. The class also has a mechanism to display the messages using the speak method. 2 of the methods are special.

The first of these is the hello world method, which represents the class constructor. The second is the underlined main method, which is a static method that implements a main driver program for the object oriented system. Relationships between classes in UML class diagrams are represented by lines connecting the classes together. These relationships fall into 2 general categories: generalization relationships, which show inheritance between classes, and association relationships, which show how instances of classes relate to each other. Each relationship in a class diagram can have multiplicity numbers assigned at 1 or both ends of the line connecting the classes.

Multiplicities can consist of a single number, such as the number 1, or a range of numbers separated by 2 dots. If the range is unbounded, the second value after the dot is a star instead of a number. Multiplicity values specify how many instances of a class are involved in a relationship. As an example, if a relationship between classes Foo and Bar has a multiplicity of 1 on the foo side and 0 star or just a single star by itself on the bar side, that relationship specifies that for every single instance of foo, there can be 0 more instances of bar in the relationship. As I mentioned before, one of the 2 categories of relationship in a UML class diagram is the generalization, which expresses the is a relationship between classes.

This relationship corresponds to inheritance in an object oriented programming language. In this example, the simple message is a type of message. In other words, the simple message class is a child class of the message parent class. This relationship is drawn by connecting a line from the simple message class to the message class with a hollow arrow at the message end pointing to the parent class. We leave off multiplicity on this type of relationship since generalizations are between classes themselves and not between instances of classes.

The first type of association relationship that I will describe is the aggregation relationship. An aggregation is a weak type of hazard relationship between class instances, in which an instance of 1 class provides a collection of instances of other classes. It is important to note that aggregation relationships are considered weak hazard relationships because the instances that are collected by the Collecting class can exist independently from the Collecting class. In practice, this means that we create instances of various classes and then add those instances to the collection, typically with a dedicated method in the collector. For example, here we have a multi message class that acts as a collection of message instances, as indicated by the relationship with the hollow diamond at the multi message end.

Multi message provides a method addMessage to add an instance of a message to the collection. This instance is already created elsewhere in the code before the add message method is called add it to the collection. If we want a class to contain instances of other classes without having to manage collections, we can use a composition relationship. A composition is a type and association that expresses a strong has a relationship between classes. In a composition, a class instance contains 1 or more instances of another class, and those instances are created or destroyed by the containing class.

It is important to understand this key distinction between an aggregation and a composition. With an aggregation, we create instances to add to the collection, then add them to the collection. With a composition, the instances are normally created in the class constructor of the composing class, and destroyed by the destructor of the composing class. There are no methods to manage the collection of composed instances. Graphically, the composition relationship is represented by a filled diamond at the composing instance end of the relationship, as shown in this example.

Here we have a double message composing instance that contains 2 instances of simple message. As shown by the multiplicity of the relationship, each instance of DoubleMessage has exactly 2 simple message instances contained within it. These simple message instances will created whenever an instance of the double message class is created and both simple message instances will be destroyed with the double message. In this lecture, I will begin discussing virtual memory. Due to the complexity of the virtual memory subsystem, the second part of this introduction will be given as a second lecture.

We have previously seen that each process in the system can be given its own logical memory space. This arrangement allows logical pages to be mapped to physical frames without the need for physical frames to be contiguous, eliminating external fragmentation and increasing the degree of multi programming that the system can support. We can further increase the degree of multi programming in the system by recognizing that processes do not actually use all the memory in their logical address spaces at any given time. Parts of the program code, including error handlers and infrequently called functions, are not utilized often. Furthermore, arrays and other data structures are often oversized and used in sections instead of all at once.

If we can swap unused pages out of memory and onto a backing store, such as a hard disk, we can fit more processes into memory at once, increasing our degree of multi programming. Furthermore, we can give each process its own large logical memory space, which can in fact be larger than the amount of physical RAM on the system. When we add a backing store, the general address translation process remains the same. Processes access memory using logical addresses, which are translated into physical addresses by the MMU. The page table is still utilized to store the page to frame mappings.

However, we do add some complexity in that a frame could be swapped out to disk at the time when it is needed. The CPU must provide a mechanism to detect the situation and generate a fault that the operating system can handle to bring the required page back into physical memory. The process of moving pages or frames of memory back and forth between RAM and the backing store is known either as swapping or as paging. Historically, the term swapping referred to the movement of entire logical address spaces or entire processes between RAM and disk. Moving single pages or frames of data between RAM and the disk was called paging.

In modern practice, both terms are used interchangeably, and the Linux kernel component that performs page movements is called the swapper. A single movement of a single page frame into or out of physical memory is called a page swap. Historically, Linux machines used a dedicated hard disk partition to store the pages that were swapped out to disk. Modern versions of Linux are just as efficient using a swap file, which is a regular file stored alongside other data in the file system. It should be noted that swapping is an optional feature, and it is possible and even quite common to run systems without any backing store or swapping capability.

Most embedded Linux systems such as Android devices do not use a backing store. If memory cannot be allocated to a process on such a system, the process typically crashes. Now page swaps are implemented by the operating system. Some assistance from hardware is required to determine when a page swap needs to be performed. When translating a page number to a frame number, the MMU checks to see if the corresponding frame is resident or loaded in RAM.

If the frame is present, the memory access proceeds as normal. If the frame is not present in RAM, however, the MMU generates a page fault, which is a CPU exception that is similar in concept to an interrupt. A specific page fault handling routine is registered with the system, either as part of the interrupt vector table or using a separate structure for fault handlers. A page fault causes this routine, known as the swapper in Linux, to be invoked. It is then the responsibility of the swapper to locate the missing page on the backing store and load it into RAM, possibly moving some other page frame to the backing store in the process.

The address translation process gains a few steps when paging is utilized. A process makes a memory request using a logical address in its private address space as usual. The MMU first checks the translation look aside buffer to determine if the page to frame mapping is present. In the case of a TLB miss, the MMU must consult the page table to find the mapping. Once the mapping from page number to frame number is known, the MMU must next verify that the page is actually loaded in physical RAM.

If the corresponding frame is available in RAM, the memory access proceeds as normal. However, if the corresponding frame is not in memory, the MMU generates a page fault, which is essentially a type of interrupt. If generated, the page fault causes the operating system to switch context to the page fault handling routine, which retrieves the corresponding memory contents from the backing store. Once this process is complete, the OS changes the CPU context back to the original process, and the memory access proceeds as normal. In order for the MMU to be able to detect situations in which a requested memory frame is not physically present in RAM, an extra bit must be added to the page table.

This bit is set to 1 whenever the contents of a logical page are present in a memory frame. If the present bit is 0, the page has been swapped out to the backing store. For efficiency reasons, the TLB entry corresponding to a row in the page table must also store the present bit. You might have noticed the terminology between page and frame is starting to become a bit blurry here. In general, we refer to pages of memory being swapped out to disk even though the swap operation is actually moving physical memory frame contents.

This fuzzy terminology is a result of historical evolution of the virtual memory subsystem. Now I'd like to take a moment to discuss the nature of backing stores as technology is changing in this area. Historically, the backing store was a mechanical hard disk drive and a number of design decisions in the virtual memory subsystems still use this assumption. However, many systems now, especially embedded systems, have only solid state storage. Since each block on a solid state drive can be erased and written only a finite number of times, there is some question as to whether it is a good idea to use an SSD as a backing store for virtual memory.

Many embedded devices do not use paging for this reason. Another issue with the backing store is that it is subject to attack via forensic disk analysis methods in the event the device is lost or stolen. Sensitive information such as cached passwords and other credentials might have been swapped out to the backing store and these pieces of information could be recovered. One solution to this problem, which is available as an easy to enable option in Mac OS 10, is to encrypt the contents of virtual memory. The downside to this approach is the addition of CPU overhead on top of the generally slow nature of the backing store hardware.

Another approach to avoiding the issues of write limits and post mortem forensic recovery of sensitive memory data is to use the Linux Compressed Caching or Comp cache mechanism as a backing store. With this approach, a section of RAM is reserved ahead of time to create a compressed RAM disk or CRAM disk. When a page is swapped out to the CRAM disk, it is compressed on the fly to fit into a smaller amount of memory. Whenever a page needs to be swapped in from the backing store, the page is read from the Z RAM disk and decompressed. Although the compression and decompression steps do result in CPU overhead, the CompCash system is still generally faster than using a disk or SSD as a backing store.

Furthermore, CompCash is as secure as RAM against forensic analysis, particularly against recovering sensitive information from a system that has been powered off for a period of time. In this lecture, I'm going to discuss object oriented design. The general purpose of object oriented design is to increase the modularity of software by breaking large software systems into smaller pieces. This approach to software engineering improves code maintainability by separating application components from each other. These components may also be reused in other software applications.

Objects in software are modeled after real world processes. For example, consider the process of playing a song. In the earliest history of music, playing a song required assembling instruments and musicians for a live performance. Fixed recordings on wire, vinyl, magnetic tape, and optical disc made songs reproducible by a physical machine. With the advent of digital music, this process is moved to software.

However, the basic operations of playing, pausing, stopping, and moving around the song remain the same. In an object oriented design, both the data required to produce the sounds that comprise the song and the behaviors required to implement the song listening process are grouped together in a logical unit. This logical unit is what we call an object in a software system. In this case, we might call this object a song player. A common misconception widely held among students, software developers, and even computer science professors is that an object oriented language such as c plus plus or Java is required to implement an object oriented design.

This is not the case. Object oriented designs are theoretically independent of the implementation language. A traditional procedural language, such as C, can be used to implement an object oriented program as long as the programmers are disciplined. Of course, object oriented languages do try to make the implementations of object oriented designs easier and less ugly. That said, it is possible to implement a regular procedural program in an object oriented language.

Just implement everything in one object. The key concept behind an object is that it combines a representation of data in a system along with algorithms to access, manipulate, and perform operations on that data. The data are encoded in an object using properties or fields in the object. The combination of the values of these properties is collectively called the state of the object. An object's behaviors are implemented as functions that are bound to the object itself.

In object oriented terminology, these functions are called methods, although c plus plus retains the older terminology of member functions. Methods perform operations on objects. Objects are created in software systems by instantiating classes. Thus, another definition of an object is that an object is an instance of a class. Classes are programming language structures that provide templates that specify how objects are to be constructed.

These classes define what properties and behaviors an object will have once it is instantiated. An issue with dividing a system into objects is that the developers must decide how data and behaviors are divided among objects. The level at which this division occurs is called granularity, and there is a direct relationship between higher granularity and the total number of objects in a system. Achieving the correct amount of granularity in an object oriented design is a balancing act. At the extreme of having too much granularity, the system is comprised of many small objects.

This type of software project becomes quite complex to develop and rather difficult to maintain. Furthermore, due to the nature of the underlying operations needed to create, manage, and destroy objects, the execution performance of an object oriented application will decrease as granularity increases. At the opposite extreme is an object oriented system that is essentially a procedural application, since all its data and behavior are implemented in a small number of huge objects, perhaps even a single ginormous object. This extreme has all the drawbacks of a non object oriented system, in that the code is not modular, is more difficult to debug, and is more difficult to maintain. Notice that maintenance tends to become a nightmare at either extreme.

A good object oriented design is somewhere in the middle of these two extremes. The benefits of object oriented design are widely celebrated among different object oriented developers. However, no one can ever seem to agree on a concise set of objectives for an object oriented system. I have followed the approach of Kaye Horstman and Gary Cornell in providing the objectives of encapsulation, information hiding, polymorphism, and reuse. Encapsulation simply means the data and methods to operate upon the data are stored together in an object.

These elements are accessed via well defined interfaces to the object. Information hiding is another desirable More importantly, other programmers can be given an interface to an object without revealing any of the implementation code, preventing them from using shortcuts or making assumptions that may later break when the object is updated. In a sense, the object is a black box. It has behaviors, but its internal workings are a mystery to outsiders. Object oriented designs provide the opportunity for polymorphic interfaces.

This means that different objects can expose compatible interfaces allowing them to be used interchangeably. Finally, well designed tightly encapsulated objects can be reused later either in the same program or in another application. In theory, over time, a collection of well tested objects will provide a development team with a library of useful components that can speed the development of new applications. In practice, of course, there is a downside to any development approach, and object oriented development is no exception. Developers working on object oriented systems have a tendency to run amok with the design, creating a terribly over engineered mess of code that is needlessly complex and overly granular.

I have been guilty of this design mistake myself. Feature creep and the realization that more features can be supported with just a few tweaks are tempting forces. Another issue with the theory of object oriented design is that objects may be difficult to reuse in practice. Object designs often wind up being tied to the underlying business process, making reuse difficult in new problem domains. Of course, one could argue that if the objects are designed to be sufficiently generic, they should be more easily reused.

However, making the objects too generic tends to result in the first problem with the designer in a muck. Finally, one major disadvantage of object oriented designs is that they are slower to execute than comparable procedural solutions. A major cause of this performance degradation is that objects are dynamically allocated and deallocated using heap memory in a process, which is a relatively slow operation. I once saw a software engineer complain about the fact that the Linux kernel uses a procedural design with go to statements in the code. Such a design is a necessity in a performance critical program like an operating system kernel.

An object oriented design would simply be too slow to execute. In this lecture, I will discuss the implementations of object oriented designs. By way of example, I will present the same object oriented design in 4 different languages: Java, C plus plus C, and Python. I will begin with a reminder about the distinction between an object oriented design and an object oriented language. It is a common misconception that an object oriented language, such as c plus plus or Java, is required to implement an object oriented design.

This is not the case. Object oriented designs are theoretically independent of the implementation language. As long as the programmers are disciplined, object oriented designs can be successfully implemented in procedural languages such as C. The primary benefit to an object oriented language is that it sometimes makes object oriented implementations easier. Some languages are better at ease of writing than are others.

My design for this example is utterly simple. I have a single class which for whatever reason I named forward. This class has a single private field, which is a string named message. The design also specifies a single public method named screen. When the screen method is invoked, the message stored in the class field will be printed to standard output.

For simplicity, this message will be set to Hello World in the class constructor. Note that I have deliberately omitted and destructor methods from the design since the syntax and semantics of these methods are tightly bound to the implementation language. Let's begin with the Java language, which is a canonical example of an object oriented language. Here I have a class named horror, which has a single private field of type string named message. A public method named scream returns no value since it is of the void type.

However, it does have the side effect of printing the message stored in the object to standard output. The constructor is the final method in the class with the signature public horror left paren right paren. This is a special method that runs whenever new objects are instantiated using this horror class as a template. In this example, the constructor sets the message for each object created from this class to a value of Hello World. I can use or reuse horror objects in another class.

Here I have a public class named driver, which has a single method named main. This main method takes an array of strings corresponding to any command line arguments to the program. This main method returns no value, as noted by its void type. Furthermore, main is a static method in Java, meaning that it can be invoked by itself directly from the class code without having to create an instance of the driver class. In other words, main is a procedural function in a language with mandatory objects.

When main executes, it instantiates a new instance of the horror class creating a new horror object. After this is created, main invokes the screen method on the horror object h, which will cause a message to be printed to standard output. Another example of an object oriented language is C plus plus In this first part of my C+ implementation of the horror class design, I define the class. Since I must manage the allocation and deallocation of memory for the message string manually in C+ I have added both constructor and destructor member functions to the class. C+ has a few major problems in terms of object support.

1st, although data and member functions are encapsulated together in runtime object instances, the implementation of the member functions is not encapsulated inside the class definition as it is with Java. Thus, C plus plus requires the use of the scope operator or the double colons, which I find less easy to read. C plus plus also introduces pointer notation that is not present in Java. Worst, C plus plus class definitions break information hiding by requiring private members to be disclosed in the class interface. Avoiding this issue requires the inconvenient use of a void typed pointer to a private implementation class.

Unlike Java, which is garbage collected, C plus plus requires the programmer to return dynamically allocated heap memory to the system manually. Thus, a destructor is needed in the class for this particular example. Whenever a new instance of horror is created, the constructor will allocate a new string on the heap containing the message Hello, World. If I omit the destructor, the memory containing the string will be leaked away whenever the horror instance is deleted. To avoid this memory leak, I need to add a destructor that deletes the dynamically allocated string before the or instance is deleted.

In a more complex object, which might dynamically allocate memory in several different member functions, the destructor would require careful coding to ensure that all utilized memory is returned to the system upon object deletion. A simple C plus plus driver program demonstrating the use of horror objects is presented here. 1st, I allocate a new horror instance on the heap. Then, I invoke the screen method to print the message to standard output. Finally, before I return from the main function, I must remember to delete the instance I created.

To be technical, if I failed to call delete, the operating system would re claim the lost memory for me whenever the program exits. However, reliance on the operating system to perform such a memory cleanup is considered bad coding practice. Okay. So we've seen that we can implement an object oriented program in an object oriented language, like Java or C plus plus But remember that I previously said that we can implement an object oriented design using a procedural language. In this example, I implement the exact same object oriented design using C.

It isn't pretty, but it is an object oriented design. The first step to implementing an object oriented design in C is to find a way to encapsulate the properties of, or data associated with an object along with the methods that operate on the object. In C, we can use a struct to accomplish this action. Since c does not have true strings, my message will be a pointer to an array of characters. Associating the screen method with horror objects requires a bit of trickery.

Here, I am using a C function pointer, or a pointer to a function in the text segment of the program to allow object instances to be associated with a particular C function it will serve as my object method. This function will not return any data, so the function pointer is given a void type. Since the c function cannot be defined right here inside the struct, I must pass the instance of the object to the function as its argument. Thus, my function will have one parameter, which will be a pointer to an instance of struct horror. Since C is not an object oriented language, it does not have a notion of a constructor.

However, I can use a factory function instead. A factory function creates a new instance of an object and returns that instance. Here I have the new horror factory function, which first allocates a struct horror data structure on the heap using the malloc function. Next, I allocate space for the message using a predefined constant to determine how long the message can be. I then use str copy to copy the literal string hello world into the message field of the new object instance.

Finally, I set the screen function pointer in the new object instance to the address of the HorrorScream function. This step binds the function, HorrorScream, as the new objects scream method. To facilitate reclaiming the heap memory, which I must do manually since C has no garbage collection, I have added a second function named delete horror. This function first frees the memory used for the message, then it frees the memory used for the horror instance itself. Note here that order is important.

I must free the message memory first before I destroy the instance. To test my C based horror code, I have a simple driver implemented as a main function. My first task inside the driver is to call my factory function to make a new instance of horror. Then, I invoke the screen method. Notice that the screen method indication looks a bit strange.

H arrow screen left paren h right paren. In order for scream to have a reference to the object on which to work, I must pass a pointer to that object as the first argument to scream. Since C is not an object oriented language, it does not have the built in syntax features for automatically mapping the object on the left side of the arrow into the method on the right side of the arrow. The programmer must perform this step manually with an extra function parameter. As with C plus plus I must reclaim the heap memory manually using my deleteHorror function.

If you compare the C code to the previous C plus plus code, you will notice quite a few similarities in the steps required to utilize the horror design. You might also notice that the implementations are approximately the same length. If I had to pick a favorite among object oriented languages, Python would be the winner, at least for single threaded code. As you can see from this complete example, which fits on one slide, the Python code is exceptionally compact and relatively easy to read. The horror class explicitly subclasses object to create a new style class instead of a classic class, which is a Python specific issue.

Class constructors in Python are always named double underscore init double underscore. Notice that the first formal parameter to any method in a class is always self, which will be a reference to the current class instance. Internally, Python is operating much like C, and passing the instance as the first parameter to the method. However, unlike C, Python has syntactic sugar for accomplishing this task. When I call h.

Scream in the driver code, I do not need to pass the instance to the method explicitly. The other item to note in this implementation is the double underscore before the message field name. These underscores are Python's approximate implementation of private scope, which uses a technique known as name mangling. Another feature of Python is that it is duck typed, which means that any 2 objects that have the same interface can be used interchangeably. Furthermore, since Python is background compiled on the fly as it is executed, the interfaces are not checked until run time.

Thus, if I were to reuse my horror class from the previous slide and add a happy class with an identical interface but a different message, I could use horror and happy interchangeably. In my driver code, I instantiate an instance of horror named h and an instance of happy named I. I then loop over both objects collected in a tuple, calling the screen method on each object. This is an ideal example of polymorphism. The same code can be written to work with 2 different types of object.

The duck typing used in Python means that I can distribute my class in compiled byte code form along with some documentation explaining the interface. I thus maintain information hiding. Since both the implementation and the instances are tightly encapsulated, Python classes can be easy to reuse. Of course, nothing is free, so there is a downside, which is that Python execution is slower than C in almost all cases. Comparison to C plus plus and Java is more difficult.

Finally, I'd like to demonstrate procedural programming in a supposedly object only language. Java is sometimes criticized for making it impossible to design non object oriented systems, and while this might technically be true, it is not a practical issue. By implementing an application using a single class in which each method is static, I can write a procedural Java program. Here I have a static add procedure that adds 2 integers together and returns an integer. In my main method, which incidentally is always static in Java, I initialize 2 primitive integer variables, x and y.

I then call the add function and print the result, the standard output. Nowhere in this code do I ever instantiate any objects with the new keyword. The system dot out object already exists and is provided by the Java virtual machine automatically. I can make Java code even more procedural by removing the main method and placing my code in a static initializer block. The only catch to this implementation is that I must call system dot exit at the end of the block.

Otherwise, the Java program would print my hello world message, then immediately crash with a no such method error exception informing me that I have no main. So to summarize, I would like you to come away from this comparative languages discussion with an understanding that object oriented designs are independent of the implementation language. You can implement an object oriented design in a procedural language, though it does require a little bit of work and a lot of discipline. You can also implement a procedural program in a mandatory object oriented language, though you might need to violate the spirit of the language to do so. In this lecture, I will discuss page replacement as it is used in the virtual memory subsystem.

I will discuss global and local page replacement, page table entries that support page replacement, and a number of classical page replacement algorithms. Whenever there is a demand for memory that is greater than the actual amount of physical RAM installed on the system, the operating system must determine which pages will be kept in memory and which pages will be swapped out to disk. When memory frame contents are swapped out to disk, the operating system needs to find a page of memory that is not currently in use. Furthermore, in an ideal case, the operating system should also pick a page that will not be used for some time, so as to reduce the total number of page swaps. In order for the page swapper to operate, some additional data must be kept about each page, including whether or not the page has been referenced and whether or not the page has been altered since it was last swapped into RAM.

Decisions regarding page swaps can be made globally or locally. With global replacement, any page in the system is a potential candidate to be swapped out in favor of another page. While simpler to implement, global page replacement does allow processes to steal memory frames from each other. On the other hand, local replacement policies allocate a limited number of frames to each process. When a process exceeds its frame allocation, only frames belonging to that process are selected for replacement.

Implementing a local page replacement algorithm is more complex than it seems on the surface, largely due to Belady's anomaly. Allocating more frames to a process does not necessarily reduce the number of page faults that occur as a result of that process. In fact, with some replacement algorithms, increasing the number of available frames actually increases the number of page faults that occur. At the opposite extreme, allocation of too few memory frames to a process also increases the number of page faults. Without enough physical memory, processes will spend more time page faulting, or swapping, than they will spend executing.

The goal with a local replacement algorithm is to find an optimal working set for each process. This working set is the minimum number of frames that a process actually requires in order to execute to some desired level of efficiency. Regardless of whether global or local replacement policies are chosen, the operating system needs a few pieces of information to implement page swapping correctly. 1st, the operating system needs to know whether or not a page that is currently in memory has been referenced by a process. Pages that are loaded but unused might be more ideal candidates to be swapped out to the backing store.

The second piece of information that needs to be stored in the page table is the dirty bit. This bit is set to 1 whenever a process writes to a page, which lets the operating system know that the copy of the memory frame contents on the backing store needs to be updated whenever the page is swapped out. Keep in mind that on systems with hardware managed page tables such as the x86 and x8664 platforms, the MMU updates these bits automatically. Whenever a page fault occurs, the operating system must locate the desired page on the backing store, then it must find a free frame and RAM into which the page can be loaded. If no memory frames are free, the operating system must select a victim frame to be swapped out to the backing store.

The algorithm that is run to determine which frame will be the victim is called the page replacement algorithm. Page replacement algorithms ideally should minimize the total number of page faults in the running system in order to maximize system performance. Let's take a look at several classic page replacement algorithms. The first classical page replacement algorithm we will consider is the random algorithm. Whenever a page swap is required, this algorithm simply picks a victim frame at random.

In practice, this random selection often picks a page that will be needed in the near future, leading to another page fault in a short time period. As such, it is not effective for minimizing page faults. Another ineffective algorithm is to select the oldest page or the page that has been in memory for the longest period of time. Unfortunately, this page could be frequently accessed, so if it is swapped out, another page fault could be triggered in a short period of time to bring it back into memory. Somewhat counterintuitively, selecting the frame that has been accessed the least frequently is also ineffective.

A page that is used relatively infrequently might still be used regularly, which would lead to another page fault to bring this frame back into RAM. The most frequently used algorithm picks whichever frame is being used the most and selects that frame to be swapped out to the backing store. This is a completely stupid idea since this page is likely to be accessed again shortly after it is swapped out. A good algorithm for choosing victim frames is the least recently used or LRU algorithm. This algorithm selects the victim frame that has not been accessed for the longest period of time.

Unfortunately, with current hardware, there is no good way to track the last memory access time. Tracking every access in software would be a terrible idea since such a scheme would require an interrupt on every memory access. Thus, it is impractical to implement LRU directly. Most implemented page replacement algorithms are approximations of LRU, however. Theoretically, the optimal algorithm or OPT is the best stage replacement algorithm to use.

With this algorithm, the operating system picks a frame that will not be accessed for the longest period of time as the victim, delaying a future page fault related to the corresponding page for as long as possible. A mathematical proof exists showing that OPT is the best possible page replacement algorithm. Unfortunately, OPT is also impossible to implement since it must be able to predict all memory accesses ahead of time. As such, we are left with LRU approximation algorithms such as the not used recently or NUR algorithm. NUR attracts frame accesses using a combination of the reference bit, dirty bit, and or an age counter.

This algorithm produces reasonable performance and actual implementations. In this lecture, which is presented in 2 parts, I will begin discussing processes. I will introduce the process model, discuss the type of information associated with the process, give an overview of process state, and introduce the concept of process forking. As is usually the case, my presentation is focused on UNIX like systems. Let's begin by defining what a process is.

A process is an instance of a computer program in execution. When we ask a computer system to run a program, the code for that program is loaded from disk into memory and executed as a process in the system. On some platforms, processes might be called jobs or tasks. On a modern system, a process consists of 1 or more threads of execution. In other words, a process can execute 1 instruction at a time, or it can execute several instructions at the same time on the CPU.

Each process on the system receives its own private allocation of resources. Each process also has access to its own data, and the operating system maintains statistics about each process in order to make effective scheduling decisions. In memory, a process is divided into segments. Program code and other read only data are placed into the text segment. Global variables in a program have their own data segment that allows both reading and writing.

Automatic variables, or local variables and functions, are allocated at compile time and placed on the stack. Data structures explicitly allocated at run time are placed on the heap. As memory is used by a process, the stack and the heap grow toward each other. If a process makes use of shared libraries, these libraries are mapped into process memory between the stack and the heap. In order to track processes correctly and allow multiple processes to share the same system, the operating system must track some information that is associated with each process.

This information includes the memory that the process is using as well as the current location in the process code that is executing, known as the process program counter. The operating system must also track other resources in use by a process, including which files are currently open in any network connections the process is using. In addition to the information generated by the process itself, the operating system must keep scheduling information and statistics about each process. This information includes a unique identifier or process ID it can be used to distinguish processes from each other. In order to arbitrate access to system resources, the operating system must also store information about the owner of a process so that permissions can be enforced correctly.

To facilitate scheduling decisions, the operating system collects various statistics about process execution, such as the amount of CPU time consumed and the amount of memory used. During the lifetime of a process, the process moves between several states. When a process is first created, it is initially in the new state. Once creation is complete and the process is ready to run, it transitions to the ready state where it waits to be assigned to a CPU core. When the scheduler selects a ready process to run, that process is moved to the running state and is given CPU resources.

During execution, a process might request external resources such as disk IO. Since these resources take time to provide, the process is moved out of the running state and into the waiting state so that the CPU core can be given to another process. Finally, when a process is finished, it is placed in the terminated state so that the operating system can perform cleanup tasks before destroying the process instance completely. In this diagram, we can see how processes may transition between states. At creation time, a process is placed into the new state while the operating system allocates initial memory and other resources.

Once creation is complete, the process is submitted to the system and placed in the ready state. Whenever a CPU core is available to execute a process, it is dispatched to the running state where it executes. Execution of a process can be interrupted for a variety of reasons. If a hardware interrupt occurs, the operating system might have to move the process off the CPU core in order to service the interrupt, returning the process to the ready state. Or the process might make an IO request, in which case the process is moved to the waiting state while the system waits on the relatively slow IO device to provide the requested data.

Once IO is complete, the process is moved back to the ready state so that it can be scheduled to run again whenever a CPU core becomes free. Upon exiting, the process is moved to the terminated state for cleanup. The mechanism for process creation is platform dependent. I will be introducing process creation on a Unix like platform, such as Linux or Mac OS 10. On these platforms, all processes descend from a single parent process that is created by the kernel at boot time.

On Linux, this first process is called init, which is the common Unix name for the first created process. Apple decided to call this process launchd on Mac OS 10. By convention, the initial process always has a process ID of 1. The initial process must also remain alive for the entire time the system is up and running. Otherwise, the whole computer crashes with the kernel panic.

The init or launchd process is started by the kernel at boot time. Every other process on the system is a child of this special process. Child processes on Unix are created by forking the parent process. The parent process makes a system call named fork, which makes a copy of the parent process. This copy, which is initially a clone of the parent, is called the child process.

It is up to the parent process to determine what, if any, resources it will share with the child process. By default, the parent process shares any open file descriptors, network connections, and other resources apart from the CPU and memory with the child. However, the program code can close or reassign resources in the child making the child completely independent of the parent. Once a child process is forked, the child becomes an independent instance of the program, which can be scheduled to run-in parallel with the parent. However, the parent process can be coded to wait on the child process to finish executing before the parent proceeds.

Furthermore, the parent process is able to terminate the child process at any time. On some systems, termination of the parent process will terminate all child processes automatically. On other systems, child processes become orphan processes whenever the parent terminates. Unix like systems, including Linux, have the ability to support both models. Any process, including a child process, has the ability to load a different program into its memory space.

This loading is accomplished via the exec system call, which replaces the entire program code of the process with the program code from a different program. New programs on UNIX like systems are started by forking an existing program, then execing the new program in the child process. When multiple processes are executing on the same system, they have the ability to execute independently or share information between themselves. An independent process is completely separate from other processes in the system. Its execution is not affected by other processes, and it cannot affect other processes as long as the operating system is designed and implemented correctly.

Alternatively, processes could share information between themselves and thereby affect each other. When this occurs, we say that the processes are cooperating. Cooperating processes may be used for a variety of reasons, including information sharing, implementing high performance parallel computation, increasing the modularity of a program implementation, or simply for convenience when implementing certain designs. In part 2 of this lecture, I will provide additional detail about process forking and executing new programs. In this lecture, I will discuss process management.

I will discuss process context, context switches, and process scheduling. Each process running on a system has a certain amount of information associated with it. A minimal set of state information that allows a process to be stopped and later restarted is called process context. Process context includes the current contents of CPU registers, the current program counter value for the process, and the contents of RAM the process is using. Switching between processes on a system is often called a context switch, although process switch is a more precise term.

The operating system could perform a context switch from a process into a section of the kernel, and then back to the same process without actually performing a process switch. Context switches require at least one mode switch to perform, since the CPU must enter supervisor mode to enter the kernel. Relatively speaking, context switches are a fairly expensive operation and frequent context switching will reduce system performance. Whenever the operating system needs to switch from one process to another, it must first make a context switch into the kernel. The kernel then saves the state of the previously running process and restores the state of the process to which it is switching.

A second context switch is then required to start the newly restored process. On Unix systems, processes are created via the fork system call. Following creation, the CPU scheduler determines when and where the process will be run. Once the CPU core is selected, the dispatcher starts the process. Whenever a process makes an IO request, a system call into the kernel is made, which removes the process from execution while waiting for the IO to complete.

The process yields any CPU cores it is currently using, so that another process can use those resources while the first process waits on the relatively slow IO device. Operations that cause the process to yield the CPU cores and wait for some external event to occur are called blocking operations. Reading data from a file is an example of such an operation. The process calls the read function, and the read function does not return until it has read some data. The process may have been moved off the CPU, and then restored and returned to the CPU while waiting for the read function to return.

Some processes are CPU bound, which means they perform large computations with few IO requests or other blocking operations. In order to enable the system to service other processes and effectively implement multi programming, the CPU scheduler must preempt these types of processes. Preemption involuntarily saves the process state, removes the process from execution, and allows another process to run. Interrupts from hardware devices may also preempt running processes in favor of kernel interrupt handlers. Without this type of preemption, the system would appear to be unresponsive to user input.

Now in order to store process state information, the operating system must maintain data structures about each process. These structures are called process control blocks or PCBs. PCBs contain fields where information about a process can be saved whenever a process is moved off the CPU. Once again, this information includes the contents of CPU registers and current program counter value. The operating system stores process control blocks in linked list structures within kernel memory space.

Here is a process control block in greater detail. Some of the information fields include process state, the unique ID for the process, the process program counter, CPU register contents, memory limits for regions of memory used by the process, open file descriptors, and other data. When performing a process switch, it is critical that the operating system saves the process's CPU state. At a theoretical minimum, this state information includes the program counter and CPU register contents. In practice, more information will be saved during each process switch.

This diagram presents a simplified view of process switching, where only the program counter and register contents are saved and restored. Here the operating system switches from the process with ID 1 to the process with ID 2. The first step in the process switch is to perform a mode switch into kernel mode, along with the context switch to the section kernel into the process control block for process 1. Also, the process state for process 1 is set to ready, indicating that the process is ready to run again once the CPU core becomes available. The process switching code then restores the CPU register contents and program counter value from the process control block for process 2.

The state of process 2 is changed from ready to running, the CPU privilege level is returned to user mode, and context to switch to process 2. Process 2 now begins executing. During the lifetime of a process, the corresponding process control block is moved between various queues in the operating system. Each queue is a linked list of process control blocks, and multiple linked lists overlap. All PCBs are at all times in the job queue, which is a linked list of all process control blocks in the system.

Process control blocks corresponding to processes in the ready state are linked into the ready list. Processes waiting for device IO have their PCBs linked into various different device SKUs. In this diagram, we can see the PCBs for 5 different processes. All PCBs are members of the job list with list links depicted by the green arrows. 3 jobs are in the ready list and 2 jobs are in a device queue waiting on IO.

The linked lists within each queue are represented by the dark blue arrows. Notice that the 2 linked lists for the queues overlap the linked list for the job list. Careful management of linked lists is a major requirement of operating system kernel code. Now I'd like to shift focus for a moment to mention scheduling since it is closely related to the linked list management. In operating systems theory, we typically divide scheduling into 3 types: job scheduling, mid term scheduling, and CPU scheduling.

Job or long term scheduling refers to the selection of processes to place into the ready state. This type of scheduler has a long interval, typically on the order of seconds to minutes. One of the clearest examples of job scheduling on modern systems occurs on high performance computational clusters, on which users submit jobs that may require hours to be scheduled and execute. Midterm scheduling refers to the swapping of inactive processes out to disk, and restoring swapped processes from disk. Many of these tasks are now part of the virtual memory subsystem.

CPU scheduling, or short term scheduling, refers to the selection of processes from the ready list to run on CPU course. This type of scheduling will be the subject of future lectures. One important operating system component related to short term scheduling is the dispatcher, which receives a process control block from the short term scheduler and restores the context of the process. The dispatcher also completes the context switch to the process by performing a mode switch into user mode and jumping to the instruction address specified by the newly restored program counter. Thanks for watching.

Please subscribe and don't miss out on new videos and lectures.